@Article{Couillet16Kernelspectralclustering,
  author           = {Couillet, Romain and Benaych-Georges, Florent},
  journal          = {Electronic Journal of Statistics},
  title            = {Kernel spectral clustering of large dimensional data},
  year             = {2016},
  issn             = {1935-7524, 1935-7524},
  month            = jan,
  number           = {1},
  pages            = {1393--1454},
  volume           = {10},
  abstract         = {This article proposes a first analysis of kernel spectral clustering methods in the regime where the dimension \$p\$ of the data vectors to be clustered and their number \$n\$ grow large at the same rate. We demonstrate, under a \$k\$-class Gaussian mixture model, that the normalized Laplacian matrix associated with the kernel matrix asymptotically behaves similar to a so-called spiked random matrix. Some of the isolated eigenvalue-eigenvector pairs in this model are shown to carry the clustering information upon a separability condition classical in spiked matrix models. We evaluate precisely the position of these eigenvalues and the content of the eigenvectors, which unveil important (sometimes quite disruptive) aspects of kernel spectral clustering both from a theoretical and practical standpoints. Our results are then compared to the actual clustering performance of images from the MNIST database, thereby revealing an important match between theory and practice.},
  creationdate     = {2022-06-06T10:15:30},
  doi              = {10.1214/16-EJS1144},
  file             = {:Couillet16 - Kernel Spectral Clustering of Large Dimensional Data.pdf:PDF},
  groups           = {bayesian chaos},
  keywords         = {15B52, 60B20, 62H30, kernel methods, Random matrix theory, spectral clustering},
  modificationdate = {2022-06-13T09:34:40},
  publisher        = {Institute of Mathematical Statistics and Bernoulli Society},
  url              = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-10/issue-1/Kernel-spectral-clustering-of-large-dimensional-data/10.1214/16-EJS1144.full},
  urldate          = {2022-06-06},
}

@Article{Alemohammad20RecurrentNeuralTangent,
  author   = {Alemohammad, Sina and Wang, Zichao and Balestriero, Randall and Baraniuk, Richard},
  journal  = {ArXiv e-prints},
  title    = {The Recurrent Neural Tangent Kernel},
  year     = {2020},
  eprint   = {2006.10246},
  file     = {:Alemohammad20 - The Recurrent Neural Tangent Kernel.pdf:PDF},
  keywords = {_tablet_modified},
}

@TechReport{Anandkumar12TensorDecompositionsLearning,
  author      = {Anandkumar, Anima and Ge, Rong and Hsu, Daniel and Kakade, Sham M. and Telgarsky, Matus},
  institution = {{Defense Technical Information Center}},
  title       = {Tensor {{Decompositions}} for {{Learning Latent Variable Models}}:},
  year        = {2012},
  address     = {{Fort Belvoir, VA}},
  month       = dec,
  abstract    = {This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models\textemdash including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation\textemdash which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models.},
  doi         = {10.21236/ADA604494},
  keywords    = {tensor-decomp},
  langid      = {english},
  shorttitle  = {Tensor {{Decompositions}} for {{Learning Latent Variable Models}}},
}

@Article{Ardizzone21ConditionalInvertibleNeural,
  author        = {Ardizzone, Lynton and Kruse, Jakob and L{\"u}th, Carsten and Bracher, Niels and Rother, Carsten and K{\"o}the, Ullrich},
  title         = {Conditional {{Invertible Neural Networks}} for {{Diverse Image-to-Image Translation}}},
  year          = {2021},
  month         = may,
  abstract      = {We introduce a new architecture called a conditional invertible neural network (cINN), and use it to address the task of diverse image-to-image translation for natural images. This is not easily possible with existing INN models due to some fundamental limitations. The cINN combines the purely generative INN model with an unconstrained feed-forward network, which efficiently preprocesses the conditioning image into maximally informative features. All parameters of a cINN are jointly optimized with a stable, maximum likelihood-based training procedure. Even though INN-based models have received far less attention in the literature than GANs, they have been shown to have some remarkable properties absent in GANs, e.g. apparent immunity to mode collapse. We find that our cINNs leverage these properties for image-to-image translation, demonstrated on day to night translation and image colorization. Furthermore, we take advantage of our bidirectional cINN architecture to explore and manipulate emergent properties of the latent space, such as changing the image style in an intuitive way.},
  archiveprefix = {arXiv},
  eprint        = {2105.02104},
  eprintclass   = {cs},
  eprinttype    = {arxiv},
  file          = {:Ardizzone21 - Conditional Invertible Neural Networks for Diverse Image to Image Translation.pdf:;:files/383/ardizzoneConditionalInvertibleNeural2021 - Conditional Invertible Neural Networks for Diverse Image to Image Translation.pdf:;:Ardizzone21 - Conditional Invertible Neural Networks for Diverse Image to Image Translation.html:},
  keywords      = {68T01, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
  primaryclass  = {cs},
}

@Article{BenDavid19RoleLayerDeep,
  author        = {{Ben-David}, Oded and Ringel, Zohar},
  title         = {The Role of a Layer in Deep Neural Networks: A {{Gaussian Process}} Perspective},
  year          = {2019},
  month         = jun,
  abstract      = {A fundamental question in deep learning concerns the role played by individual layers in a deep neural network (DNN) and the transferable properties of the data representations which they learn. To the extent that layers have clear roles one should be able to optimize them separately using layer-wise loss functions. Such loss functions would describe what is the set of good data representations at each depth of the network and provide a target for layer-wise greedy optimization (LEGO). Here we derive a novel correspondence between Gaussian Processes and SGD trained deep neural networks. Leveraging this correspondence, we derive the Deep Gaussian Layer-wise loss functions (DGLs) which, we believe, are the first supervised layer-wise loss functions which are both explicit and competitive in terms of accuracy. Being highly structured and symmetric, the DGLs provide a promising analytic route to understanding the internal representations generated by DNNs.},
  archiveprefix = {arXiv},
  eprint        = {1902.02354},
  eprintclass   = {cond-mat,stat},
  eprinttype    = {arxiv},
  file          = {:Ben-David19 - The Role of a Layer in Deep Neural Networks_ a Gaussian Process Perspective.pdf:PDF},
  keywords      = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cond-mat, stat},
  shorttitle    = {The Role of a Layer in Deep Neural Networks},
}

@Article{Bordelon20SpectrumDependentLearning,
  author   = {Bordelon, Blake and Canatar, Abdulkadir and Pehlevan, Cengiz},
  journal  = {ArXiv e-prints},
  title    = {Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks},
  year     = {2020},
  eprint   = {2002.02561},
  file     = {:Bordelon20 - Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks.pdf:PDF},
  keywords = {_tablet},
}

@Article{Bradde17PCAMeetsRG,
  author   = {Bradde, Serena and Bialek, William},
  journal  = {Journal of Statistical Physics},
  title    = {{{PCA Meets RG}}},
  year     = {2017},
  issn     = {0022-4715, 1572-9613},
  month    = may,
  number   = {3-4},
  pages    = {462--475},
  volume   = {167},
  abstract = {A system with many degrees of freedom can be characterized by a covariance matrix; principal components analysis focuses on the eigenvalues of this matrix, hoping to find a lower dimensional description. But when the spectrum is nearly continuous, any distinction between components that we keep and those that we ignore becomes arbitrary; it then is natural to ask what happens as we vary this arbitrary cutoff. We argue that this problem is analogous to the momentum shell renormalization group. Following this analogy, we can define relevant and irrelevant operators, where the role of dimensionality is played by properties of the eigenvalue density. These results also suggest an approach to the analysis of real data. As an example, we study neural activity in the vertebrate retina as it responds to naturalistic movies, and find evidence of behavior controlled by a nontrivial fixed point. Applied to financial data, our analysis separates modes dominated by sampling noise from a smaller but still macroscopic number of modes described by a non-Gaussian distribution.},
  doi      = {10.1007/s10955-017-1770-6},
  langid   = {english},
}

@Article{Braun05SpectralPropertiesKernel,
  author    = {Braun, Mikio Ludwig},
  title     = {Spectral Properties of the Kernel Matrix and Their Relation to Kernel Methods in Machine Learning},
  year      = {2005},
  file      = {:Braun05 - Spectral Properties of the Kernel Matrix and Their Relation to Kernel Methods in Machine Learning.pdf:},
  publisher = {{Universit\"ats-und Landesbibliothek Bonn}},
}

@Article{Canatar20StatisticalMechanicsGeneralization,
  author  = {Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
  journal = {ArXiv e-prints},
  title   = {Statistical Mechanics of Generalization in Kernel Regression},
  year    = {2020},
  eprint  = {2006.13198},
}

@Article{Canatar21SpectralBiasTask,
  author           = {Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
  journal          = {Nature Communications},
  title            = {Spectral {{Bias}} and {{Task-Model Alignment Explain Generalization}} in {{Kernel Regression}} and {{Infinitely Wide Neural Networks}}},
  year             = {2021},
  issn             = {2041-1723},
  month            = dec,
  number           = {1},
  pages            = {2914},
  volume           = {12},
  abstract         = {Generalization beyond a training dataset is a main goal of machine learning, but theoretical understanding of generalization remains an open problem for many models. The need for a new theory is exacerbated by recent observations in deep neural networks where overparameterization leads to better performance, contradicting the conventional wisdom from classical statistics. In this paper, we investigate generalization error for kernel regression, which, besides being a popular machine learning method, also includes infinitely overparameterized neural networks trained with gradient descent. We use techniques from statistical mechanics to derive an analytical expression for generalization error applicable to any kernel or data distribution. We present applications of our theory to real and synthetic datasets, and for many kernels including those that arise from training deep neural networks in the infinite-width limit. We elucidate an inductive bias of kernel regression to explain data with "simple functions", which are identified by solving a kernel eigenfunction problem on the data distribution. This notion of simplicity allows us to characterize whether a kernel is compatible with a learning task, facilitating good generalization performance from a small number of training examples. We show that more data may impair generalization when noisy or not expressible by the kernel, leading to non-monotonic learning curves with possibly many peaks. To further understand these phenomena, we turn to the broad class of rotation invariant kernels, which is relevant to training deep neural networks in the infinite-width limit, and present a detailed mathematical analysis of them when data is drawn from a spherically symmetric distribution and the number of input dimensions is large.},
  archiveprefix    = {arXiv},
  doi              = {10.1038/s41467-021-23103-1},
  eprint           = {2006.13198},
  eprinttype       = {arxiv},
  file             = {:Canatar21 - Spectral Bias and Task Model Alignment Explain Generalization in Kernel Regression and Infinitely Wide Neural Networks.pdf:PDF},
  groups           = {bayesian chaos},
  keywords         = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Statistics - Machine Learning},
  modificationdate = {2022-06-02T15:51:10},
  readstatus       = {read},
}

@Article{Chung18ClassificationGeometryGeneral,
  author           = {Chung, SueYeon and Lee, Daniel D. and Sompolinsky, Haim},
  journal          = {Physical Review X},
  title            = {Classification and {{Geometry}} of {{General Perceptual Manifolds}}},
  year             = {2018},
  issn             = {2160-3308},
  month            = jul,
  number           = {3},
  pages            = {031003},
  volume           = {8},
  doi              = {10.1103/PhysRevX.8.031003},
  file             = {:Chung18 - Classification and Geometry of General Perceptual Manifolds.pdf:PDF},
  keywords         = {_tablet_modified},
  langid           = {english},
  modificationdate = {2022-04-02T15:01:58},
  priority         = {prio1},
}

@Article{Cover65GeometricalStatisticalProperties,
  author           = {Cover, Thomas M.},
  journal          = {IEEE Transactions on Electronic Computers},
  title            = {Geometrical and {{Statistical Properties}} of {{Systems}} of {{Linear Inequalities}} with {{Applications}} in {{Pattern Recognition}}},
  year             = {1965},
  issn             = {0367-7508},
  month            = jun,
  number           = {3},
  pages            = {326--334},
  volume           = {EC-14},
  abstract         = {This paper develops the separating capacities of families of nonlinear decision surfaces by a direct application of a theorem in classical combinatorial geometry. It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes. Applying these ideas to the vertices of a binary n-cube yields bounds on the number of spherically, quadratically, and, in general, nonlinearly separable Boolean functions of n variables.},
  doi              = {10.1109/PGEC.1965.264137},
  file             = {:Cover65 - Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition.pdf:PDF},
  groups           = {bayesian chaos},
  langid           = {english},
  modificationdate = {2022-04-24T22:12:05},
}

@Article{Dong20ReservoirComputingMeets,
  author  = {Dong, Jonathan and Ohana, Ruben and Rafayelyan, Mushegh and Krzakala, Florent},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Reservoir {{Computing}} Meets {{Recurrent Kernels}} and {{Structured Transforms}}},
  year    = {2020},
  volume  = {33},
}

@Article{Duvenaud14AutomaticModelConstruction,
  author           = {Duvenaud, David Kristjanson},
  journal          = {PhD thesis, University of Cambridge},
  title            = {Automatic {{Model Construction}} with {{Gaussian Processes}}},
  year             = {2014},
  pages            = {157},
  groups           = {bayesian chaos},
  langid           = {english},
  modificationdate = {2022-06-13T09:48:27},
}

@Article{Fischer22DecomposingNeuralNetworks,
  author        = {Fischer, Kirsten and Ren{\'e}, Alexandre and Keup, Christian and Layer, Moritz and Dahmen, David and Helias, Moritz},
  title         = {Decomposing Neural Networks as Mappings of Correlation Functions},
  year          = {2022},
  month         = feb,
  abstract      = {Understanding the functional principles of information processing in deep neural networks continues to be a challenge, in particular for networks with trained and thus non-random weights. To address this issue, we study the mapping between probability distributions implemented by a deep feed-forward network. We characterize this mapping as an iterated transformation of distributions, where the non-linearity in each layer transfers information between different orders of correlation functions. This allows us to identify essential statistics in the data, as well as different information representations that can be used by neural networks. Applied to an XOR task and to MNIST, we show that correlations up to second order predominantly capture the information processing in the internal layers, while the input layer also extracts higher-order correlations from the data. This analysis provides a quantitative and explainable perspective on classification.},
  archiveprefix = {arXiv},
  eprint        = {2202.04925},
  eprintclass   = {cond-mat,stat},
  eprinttype    = {arxiv},
  file          = {:Fischer22 - Decomposing Neural Networks As Mappings of Correlation Functions.pdf:;:Fischer22 - Decomposing Neural Networks As Mappings of Correlation Functions.html:},
  keywords      = {Condensed Matter - Disordered Systems and Neural Networks, Statistics - Machine Learning},
  primaryclass  = {cond-mat, stat},
}

@Article{Fort20DeepLearningversus,
  author  = {Fort, Stanislav and Dziugaite, Gintare Karolina and Paul, Mansheej and Kharaghani, Sepideh and Roy, Daniel M. and Ganguli, Surya},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Deep Learning versus Kernel Learning: An Empirical Study of Loss Landscape Geometry and the Time Evolution of the {{Neural Tangent Kernel}}},
  year    = {2020},
  pages   = {5850--5861},
  volume  = {33},
}

@InProceedings{Ghahramani16HistoryBayesianNeural,
  author    = {Ghahramani, Zoubin},
  booktitle = {{{NIPS}} Workshop on {{Bayesian}} Deep Learning},
  title     = {A History of Bayesian Neural Networks},
  year      = {2016},
}

@Article{Ghorbani20WhenDoNeural,
  author           = {Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal          = {Advances in Neural Information Processing Systems},
  title            = {When Do Neural Networks Outperform Kernel Methods?},
  year             = {2020},
  pages            = {14820--14830},
  volume           = {33},
  file             = {:Ghorbani20 - When Do Neural Networks Outperform Kernel Methods_.pdf:PDF},
  modificationdate = {2022-04-02T15:04:21},
  priority         = {prio1},
}

@Article{Goldt21GaussianEquivalenceGenerative,
  author        = {Goldt, Sebastian and Loureiro, Bruno and Reeves, Galen and Krzakala, Florent and M{\'e}zard, Marc and Zdeborov{\'a}, Lenka},
  title         = {The {{Gaussian}} Equivalence of Generative Models for Learning with Shallow Neural Networks},
  year          = {2021},
  month         = may,
  abstract      = {Understanding the impact of data structure on the computational tractability of learning is a key challenge for the theory of neural networks. Many theoretical works do not explicitly model training data, or assume that inputs are drawn component-wise independently from some simple probability distribution. Here, we go beyond this simple paradigm by studying the performance of neural networks trained on data drawn from pre-trained generative models. This is possible due to a Gaussian equivalence stating that the key metrics of interest, such as the training and test errors, can be fully captured by an appropriately chosen Gaussian model. We provide three strands of rigorous, analytical and numerical evidence corroborating this equivalence. First, we establish rigorous conditions for the Gaussian equivalence to hold in the case of single-layer generative models, as well as deterministic rates for convergence in distribution. Second, we leverage this equivalence to derive a closed set of equations describing the generalisation performance of two widely studied machine learning problems: two-layer neural networks trained using one-pass stochastic gradient descent, and full-batch pre-learned features or kernel methods. Finally, we perform experiments demonstrating how our theory applies to deep, pre-trained generative models. These results open a viable path to the theoretical study of machine learning models with realistic data.},
  archiveprefix = {arXiv},
  eprint        = {2006.14709},
  eprintclass   = {cond-mat,stat},
  eprinttype    = {arxiv},
  file          = {:Goldt21 - The Gaussian Equivalence of Generative Models for Learning with Shallow Neural Networks.pdf:},
  groups        = {RNFlows JC},
  keywords      = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cond-mat, stat},
}

@Article{Goldt19GeneralisationDynamicsOnline,
  author        = {Goldt, Sebastian and Advani, Madhu S. and Saxe, Andrew M. and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  title         = {Generalisation Dynamics of Online Learning in Over-Parameterised Neural Networks},
  year          = {2019},
  month         = jan,
  abstract      = {Deep neural networks achieve stellar generalisation on a variety of problems, despite often being large enough to easily fit all their training data. Here we study the generalisation dynamics of two-layer neural networks in a teacher-student setup, where one network, the student, is trained using stochastic gradient descent (SGD) on data generated by another network, called the teacher. We show how for this problem, the dynamics of SGD are captured by a set of differential equations. In particular, we demonstrate analytically that the generalisation error of the student increases linearly with the network size, with other relevant parameters held constant. Our results indicate that achieving good generalisation in neural networks depends on the interplay of at least the algorithm, its learning rate, the model architecture, and the data set.},
  archiveprefix = {arXiv},
  eprint        = {1901.09085},
  eprintclass   = {cond-mat,stat},
  eprinttype    = {arxiv},
  file          = {:Goldt19 - Generalisation Dynamics of Online Learning in Over Parameterised Neural Networks.pdf:PDF},
  groups        = {RNFlows JC},
  keywords      = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cond-mat, stat},
}

@Article{Grosvenor22EdgeChaosQuantum,
  author           = {Grosvenor, Kevin T. and Jefferson, Ro},
  title            = {The Edge of Chaos: Quantum Field Theory and Deep Neural Networks},
  year             = {2022},
  month            = jan,
  abstract         = {We explicitly construct the quantum field theory corresponding to a general class of deep neural networks encompassing both recurrent and feedforward architectures. We first consider the mean-field theory (MFT) obtained as the leading saddlepoint in the action, and derive the condition for criticality via the largest Lyapunov exponent. We then compute the loop corrections to the correlation function in a perturbative expansion in the ratio of depth \$T\$ to width \$N\$, and find a precise analogy with the well-studied \$O(N)\$ vector model, in which the variance of the weight initializations plays the role of the 't Hooft coupling. In particular, we compute both the \$\textbackslash mathcal\{O\}(1)\$ corrections quantifying fluctuations from typicality in the ensemble of networks, and the subleading \$\textbackslash mathcal\{O\}(T/N)\$ corrections due to finite-width effects. These provide corrections to the correlation length that controls the depth to which information can propagate through the network, and thereby sets the scale at which such networks are trainable by gradient descent. Our analysis provides a first-principles approach to the rapidly emerging NN-QFT correspondence, and opens several interesting avenues to the study of criticality in deep neural networks.},
  archiveprefix    = {arXiv},
  eprint           = {2109.13247},
  eprintclass      = {cond-mat,physics:hep-th,stat},
  eprinttype       = {arxiv},
  file             = {:Grosvenor22 - The Edge of Chaos_ Quantum Field Theory and Deep Neural Networks.pdf:;:Grosvenor22 - The Edge of Chaos_ Quantum Field Theory and Deep Neural Networks.html:},
  keywords         = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, High Energy Physics - Theory, Statistics - Machine Learning},
  modificationdate = {2022-04-10T18:24:01},
  primaryclass     = {cond-mat, physics:hep-th, stat},
  readstatus       = {read},
  shorttitle       = {The Edge of Chaos},
}

@Article{Halko10FindingStructureRandomness,
  author        = {Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A.},
  title         = {Finding Structure with Randomness: {{Probabilistic}} Algorithms for Constructing Approximate Matrix Decompositions},
  year          = {2010},
  month         = dec,
  abstract      = {Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed---either explicitly or implicitly---to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, speed, and robustness. These claims are supported by extensive numerical experiments and a detailed error analysis.},
  archiveprefix = {arXiv},
  eprint        = {0909.4061},
  eprintclass   = {math},
  eprinttype    = {arxiv},
  file          = {:Halko10 - Finding Structure with Randomness_ Probabilistic Algorithms for Constructing Approximate Matrix Decompositions.pdf:;:Halko10 - Finding Structure with Randomness_ Probabilistic Algorithms for Constructing Approximate Matrix Decompositions.html:},
  keywords      = {Mathematics - Numerical Analysis, Mathematics - Probability},
  primaryclass  = {math},
  shorttitle    = {Finding Structure with Randomness},
}

@Article{HeBayesianDeepEnsembles,
  author   = {He, Bobby and Lakshminarayanan, Balaji and Teh, Yee Whye},
  title    = {Bayesian {{Deep Ensembles}} via the {{Neural Tangent Kernel}}},
  pages    = {13},
  abstract = {We explore the link between deep ensembles and Gaussian processes (GPs) through the lens of the Neural Tangent Kernel (NTK): a recent development in understanding the training dynamics of wide neural networks (NNs). Previous work has shown that even in the infinite width limit, when NNs become GPs, there is no GP posterior interpretation to a deep ensemble trained with squared error loss. We introduce a simple modification to standard deep ensembles training, through addition of a computationally-tractable, randomised and untrainable function to each ensemble member, that enables a posterior interpretation in the infinite width limit. When ensembled together, our trained NNs give an approximation to a posterior predictive distribution, and we prove that our Bayesian deep ensembles make more conservative predictions than standard deep ensembles in the infinite width limit. Finally, using finite width NNs we demonstrate that our Bayesian deep ensembles faithfully emulate the analytic posterior predictive when available, and can outperform standard deep ensembles in various out-of-distribution settings, for both regression and classification tasks.},
  file     = {:He - Bayesian Deep Ensembles Via the Neural Tangent Kernel.pdf:PDF},
  keywords = {_tablet_modified},
  langid   = {english},
}

@Article{Helias19StatisticalFieldTheory,
  author        = {Helias, Moritz and Dahmen, David},
  title         = {Statistical Field Theory for Neural Networks},
  year          = {2019},
  month         = jan,
  abstract      = {These notes attempt a self-contained introduction into statistical field theory applied to neural networks of rate units and binary spins. The presentation consists of three parts: First, the introduction of fundamental notions of probabilities, moments, cumulants, and their relation by the linked cluster theorem, of which Wick's theorem is the most important special case; followed by the diagrammatic formulation of perturbation theory, reviewed in the statistical setting. Second, dynamics described by stochastic differential equations in the Ito-formulation, treated in the Martin-Siggia-Rose-De Dominicis-Janssen path integral formalism. With concepts from disordered systems, we then study networks with random connectivity and derive their self-consistent dynamic mean-field theory, explaining the statistics of fluctuations and the emergence of different phases with regular and chaotic dynamics. Third, we introduce the effective action, vertex functions, and the loopwise expansion. These tools are illustrated by systematic derivations of self-consistency equations, going beyond the mean-field approximation. These methods are applied to the pairwise maximum entropy (Ising spin) model, including the recently-found diagrammatic derivation of the Thouless-Anderson-Palmer mean field theory.},
  archiveprefix = {arXiv},
  eprint        = {1901.10416},
  eprintclass   = {cond-mat},
  eprinttype    = {arxiv},
  file          = {:Helias19 - Statistical Field Theory for Neural Networks.pdf:;:Helias19 - Statistical Field Theory for Neural Networks.html:},
  keywords      = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics},
  primaryclass  = {cond-mat},
}

@Article{Jacot18NeuralTangentKernel,
  author           = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal          = {ArXiv e-prints},
  title            = {Neural Tangent Kernel: {{Convergence}} and Generalization in Neural Networks},
  year             = {2018},
  month            = jun,
  eprint           = {1806.07572},
  file             = {:Jacot18 - Neural Tangent Kernel_ Convergence and Generalization in Neural Networks.pdf:PDF},
  groups           = {bayesian chaos},
  modificationdate = {2022-03-16T08:26:58},
}

@Article{JonaLasinio01RenormalizationGroupProbability,
  author        = {{Jona-Lasinio}, Giovanni},
  journal       = {Physics Reports},
  title         = {Renormalization {{Group}} and {{Probability Theory}}},
  year          = {2001},
  issn          = {0370-1573},
  month         = oct,
  number        = {4-6},
  pages         = {439--458},
  volume        = {352},
  abstract      = {The renormalization group has played an important role in the physics of the second half of the twentieth century both as a conceptual and a calculational tool. In particular it provided the key ideas for the construction of a qualitative and quantitative theory of the critical point in phase transitions and started a new era in statistical mechanics. Probability theory lies at the foundation of this branch of physics and the renormalization group has an interesting probabilistic interpretation as it was recognized in the middle seventies. This paper intends to provide a concise introduction to this aspect of the theory of phase transitions which clarifies the deep statistical significance of critical universality.},
  archiveprefix = {arXiv},
  doi           = {10.1016/S0370-1573(01)00042-4},
  eprint        = {cond-mat/0009219},
  eprinttype    = {arxiv},
  file          = {:Jona-Lasinio01 - Renormalization Group and Probability Theory.pdf:;:Jona-Lasinio01 - Renormalization Group and Probability Theory.html:},
  keywords      = {Condensed Matter},
}

@Article{Karaletsos20HierarchicalGaussianProcess,
  author  = {Karaletsos, Theofanis and Bui, Thang D.},
  journal = {ArXiv e-prints},
  title   = {Hierarchical Gaussian Process Priors for Bayesian Neural Network Weights},
  year    = {2020},
  eprint  = {2002.04033},
}

@Article{Keup21TransientChaoticDimensionality,
  author           = {Keup, Christian and K{\"u}hn, Tobias and Dahmen, David and Helias, Moritz},
  journal          = {Physical Review X},
  title            = {Transient {{Chaotic Dimensionality Expansion}} by {{Recurrent Networks}}},
  year             = {2021},
  issn             = {2160-3308},
  month            = jun,
  number           = {2},
  pages            = {021064},
  volume           = {11},
  doi              = {10.1103/PhysRevX.11.021064},
  file             = {:Keup21 - Transient Chaotic Dimensionality Expansion by Recurrent Networks.pdf:PDF},
  groups           = {bayesian chaos},
  langid           = {english},
  modificationdate = {2022-05-04T14:53:50},
}

@Article{KochJanusz18MutualInformationNeural,
  author   = {{Koch-Janusz}, Maciej and Ringel, Zohar},
  journal  = {Nature Physics},
  title    = {Mutual Information, Neural Networks and the Renormalization Group},
  year     = {2018},
  pages    = {5},
  volume   = {14},
  file     = {:Koch-Janusz18 - Mutual Information, Neural Networks and the Renormalization Group.pdf:PDF},
  keywords = {_tablet_modified},
  langid   = {english},
}

@Article{Kriener14HowPatternFormation,
  author  = {Kriener, Birgit and Helias, Moritz and Rotter, Stefan and Diesmann, Markus and Einevoll, Gaute T.},
  journal = {Frontiers in Computational Neuroscience},
  title   = {How Pattern Formation in Ring Networks of Excitatory and Inhibitory Spiking Neurons Depends on the Input Current Regime},
  year    = {2014},
  issn    = {1662-5188},
  volume  = {7},
  doi     = {10.3389/fncom.2013.00187},
  file    = {:Kriener14 - How Pattern Formation in Ring Networks of Excitatory and Inhibitory Spiking Neurons Depends on the Input Current Regime.pdf:},
}

@Article{Lee17DeepNeuralNetworks,
  author   = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and {Sohl-Dickstein}, Jascha},
  journal  = {ArXiv e-prints},
  title    = {Deep Neural Networks as Gaussian Processes},
  year     = {2017},
  eprint   = {1711.00165},
  file     = {:Lee17 - Deep Neural Networks As Gaussian Processes.pdf:PDF},
  keywords = {_tablet},
}

@Article{Lee19WideNeuralNetworks,
  author           = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and {Sohl-Dickstein}, Jascha and Pennington, Jeffrey},
  journal          = {Advances in neural information processing systems},
  title            = {Wide Neural Networks of Any Depth Evolve as Linear Models under Gradient Descent},
  year             = {2019},
  pages            = {8572--8583},
  volume           = {32},
  comment          = {NNGP and NTK are linear models},
  file             = {:Lee19 - Wide Neural Networks of Any Depth Evolve As Linear Models under Gradient Descent.pdf:PDF},
  modificationdate = {2022-04-12T11:30:13},
}

@Article{Lee20FiniteversusInfinite,
  author  = {Lee, Jaehoon and Schoenholz, Samuel and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and {Sohl-Dickstein}, Jascha},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Finite versus Infinite Neural Networks: An Empirical Study},
  year    = {2020},
  pages   = {15156--15172},
  volume  = {33},
}

@Article{Lee21ScaleInvariantRepresentation,
  author        = {Lee, Sungyeop and Jo, Junghyo},
  title         = {Scale-Invariant Representation of Machine Learning},
  year          = {2021},
  month         = sep,
  abstract      = {The success of machine learning stems from its structured data representation. Similar data have close representation as compressed codes for classification or emerged labels for clustering. We observe that the frequency of the internal representation follows power laws in both supervised and unsupervised learning. The scale-invariant distribution implies that machine learning largely compresses frequent typical data, and at the same time, differentiates many atypical data as outliers. In this study, we derive how the power laws can naturally arise in machine learning. In terms of information theory, the scale-invariant representation corresponds to a maximally uncertain data grouping among possible representations that guarantee pre-specified learning accuracy.},
  archiveprefix = {arXiv},
  eprint        = {2109.02914},
  eprintclass   = {physics},
  eprinttype    = {arxiv},
  keywords      = {Computer Science - Information Theory, Computer Science - Machine Learning, Physics - Data Analysis; Statistics and Probability},
  langid        = {english},
  primaryclass  = {physics},
}

@Article{MacKay98IntroductionGaussianProcesses,
  author  = {MacKay, David JC and others},
  journal = {NATO ASI series F computer and systems sciences},
  title   = {Introduction to Gaussian Processes},
  year    = {1998},
  pages   = {133--166},
  volume  = {168},
}

@Article{Millidge20PredictiveCodingApproximates,
  author        = {Millidge, Beren and Tschantz, Alexander and Buckley, Christopher L.},
  title         = {Predictive {{Coding Approximates Backprop}} along {{Arbitrary Computation Graphs}}},
  year          = {2020},
  month         = oct,
  abstract      = {Backpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. However, backprop is often criticised for lacking biological plausibility. Recently, it has been shown that backprop in multilayer-perceptrons (MLPs) can be approximated using predictive coding, a biologically-plausible process theory of cortical computation which relies only on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs, but rather in the concept of automatic differentiation which allows for the optimisation of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding CNNs, RNNs, and the more complex LSTMs, which include a non-layer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks, while utilising only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry, and may also contribute to the development of completely distributed neuromorphic architectures.},
  archiveprefix = {arXiv},
  eprint        = {2006.04182},
  eprintclass   = {cs},
  eprinttype    = {arxiv},
  file          = {:Millidge20 - Predictive Coding Approximates Backprop along Arbitrary Computation Graphs.pdf:;:Millidge20 - Predictive Coding Approximates Backprop along Arbitrary Computation Graphs.html:},
  keywords      = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  primaryclass  = {cs},
}

@Article{Nassar201/nNeuralRepresentation,
  author        = {Nassar, Josue and Sokol, Piotr Aleksander and Chung, SueYeon and Harris, Kenneth D. and Park, Il Memming},
  title         = {On 1/n Neural Representation and Robustness},
  year          = {2020},
  month         = dec,
  abstract      = {Understanding the nature of representation in neural networks is a goal shared by neuroscience and machine learning. It is therefore exciting that both fields converge not only on shared questions but also on similar approaches. A pressing question in these areas is understanding how the structure of the representation used by neural networks affects both their generalization, and robustness to perturbations. In this work, we investigate the latter by juxtaposing experimental results regarding the covariance spectrum of neural representations in the mouse V1 (Stringer et al) with artificial neural networks. We use adversarial robustness to probe Stringer et al's theory regarding the causal role of a 1/n covariance spectrum. We empirically investigate the benefits such a neural code confers in neural networks, and illuminate its role in multi-layer architectures. Our results show that imposing the experimentally observed structure on artificial neural networks makes them more robust to adversarial attacks. Moreover, our findings complement the existing theory relating wide neural networks to kernel methods, by showing the role of intermediate representations.},
  archiveprefix = {arXiv},
  eprint        = {2012.04729},
  eprintclass   = {cs},
  eprinttype    = {arxiv},
  file          = {:Nassar20 - On 1_n Neural Representation and Robustness.pdf:;:files/454/nassarNeuralRepresentationRobustness2020 - On 1_n Neural Representation and Robustness.pdf:;:Nassar20 - On 1_n Neural Representation and Robustness.html:},
  keywords      = {Computer Science - Machine Learning},
  primaryclass  = {cs},
}

@Book{Neal96BayesianLearningNeural,
  author    = {Neal, Radford M.},
  publisher = {{Springer, New York, NY}},
  title     = {Bayesian Learning for Neural Networks},
  year      = {1996},
  address   = {{New York, NY, USA}},
  doi       = {10.1007/978-1-4612-0745-0},
  issn      = {0930-0325},
}

@InProceedings{Novak19BayesianDeepConvolutional,
  author    = {Novak, Roman and Xiao, Lechao and Bahri, Yasaman and Lee, Jaehoon and Yang, Greg and Abolafia, Daniel A. and Pennington, Jeffrey and {Sohl-dickstein}, Jascha},
  booktitle = {International Conference on Learning Representations},
  title     = {Bayesian Deep Convolutional Networks with Many Channels Are Gaussian Processes},
  year      = {2019},
}

@Article{Pennington17ResurrectingSigmoidDeep,
  author        = {Pennington, Jeffrey and Schoenholz, Samuel S. and Ganguli, Surya},
  title         = {Resurrecting the Sigmoid in Deep Learning through Dynamical Isometry: Theory and Practice},
  year          = {2017},
  month         = nov,
  abstract      = {It is well known that the initialization of weights in deep neural networks can have a dramatic impact on learning speed. For example, ensuring the mean squared singular value of a network's input-output Jacobian is \$O(1)\$ is essential for avoiding the exponential vanishing or explosion of gradients. The stronger condition that all singular values of the Jacobian concentrate near \$1\$ is a property known as dynamical isometry. For deep linear networks, dynamical isometry can be achieved through orthogonal weight initialization and has been shown to dramatically speed up learning; however, it has remained unclear how to extend these results to the nonlinear setting. We address this question by employing powerful tools from free probability theory to compute analytically the entire singular value distribution of a deep network's input-output Jacobian. We explore the dependence of the singular value distribution on the depth of the network, the weight initialization, and the choice of nonlinearity. Intriguingly, we find that ReLU networks are incapable of dynamical isometry. On the other hand, sigmoidal networks can achieve isometry, but only with orthogonal weight initialization. Moreover, we demonstrate empirically that deep nonlinear networks achieving dynamical isometry learn orders of magnitude faster than networks that do not. Indeed, we show that properly-initialized deep sigmoidal networks consistently outperform deep ReLU networks. Overall, our analysis reveals that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning.},
  archiveprefix = {arXiv},
  eprint        = {1711.04735},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  file          = {:Pennington17 - Resurrecting the Sigmoid in Deep Learning through Dynamical Isometry_ Theory and Practice.pdf:;:Pennington17 - Resurrecting the Sigmoid in Deep Learning through Dynamical Isometry_ Theory and Practice.html:},
  keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
  primaryclass  = {cs, stat},
  shorttitle    = {Resurrecting the Sigmoid in Deep Learning through Dynamical Isometry},
}

@Article{Poole16ExponentialExpressivityDeep,
  author           = {Poole, Ben and Lahiri, Subhaneil and Raghu, Maithra and {Sohl-Dickstein}, Jascha and Ganguli, Surya},
  title            = {Exponential Expressivity in Deep Neural Networks through Transient Chaos},
  year             = {2016},
  month            = jun,
  abstract         = {We combine Riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in generic, deep neural networks with random weights. Our results reveal an order-to-chaos expressivity phase transition, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth but not width. We prove this generic class of deep random functions cannot be efficiently computed by any shallow network, going beyond prior work restricted to the analysis of single functions. Moreover, we formalize and quantitatively demonstrate the long conjectured idea that deep networks can disentangle highly curved manifolds in input space into flat manifolds in hidden space. Our theoretical analysis of the expressive power of deep networks broadly applies to arbitrary nonlinearities, and provides a quantitative underpinning for previously abstract notions about the geometry of deep functions.},
  archiveprefix    = {arXiv},
  eprint           = {1606.05340},
  eprintclass      = {cond-mat,stat},
  eprinttype       = {arxiv},
  file             = {:Poole16 - Exponential Expressivity in Deep Neural Networks through Transient Chaos.pdf:PDF},
  groups           = {PDLT},
  keywords         = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Statistics - Machine Learning},
  modificationdate = {2022-04-02T18:01:43},
  primaryclass     = {cond-mat, stat},
  priority         = {prio1},
}

@Book{Rasmussen06GaussianProcessesMachine,
  author     = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  publisher  = {{MIT Press}},
  title      = {Gaussian Processes for Machine Learning},
  year       = {2006},
  address    = {{Cambridge, Mass}},
  isbn       = {978-0-262-18253-9},
  series     = {Adaptive Computation and Machine Learning},
  annotation = {OCLC: ocm61285753},
  file       = {:Rasmussen06 - Gaussian Processes for Machine Learning.pdf:},
  keywords   = {Data processing, Gaussian processes, Machine learning, Mathematical models},
  langid     = {english},
  lccn       = {QA274.4 .R37 2006},
}

@Book{Rayleigh96TheorySound,
  author    = {Rayleigh, John William Strutt Baron},
  publisher = {{Macmillan}},
  title     = {The Theory of Sound},
  year      = {1896},
  volume    = {2},
}

@Book{RobertsPrinciplesDeepLearning,
  author           = {Roberts, Daniel A. and Yaida, Sho and Hanin, Boris},
  publisher        = {Cambridge University Press},
  title            = {The Principles of Deep Learning Theory},
  year             = {2022},
  note             = {\url{https://deeplearningtheory.com}},
  archiveprefix    = {arXiv},
  eprint           = {2106.10165},
  file             = {:Roberts - The Principles of Deep Learning Theory.pdf:PDF},
  groups           = {PDLT},
  langid           = {english},
  modificationdate = {2022-04-28T16:37:33},
  primaryclass     = {cs.LG},
}

@Article{Schoenholz17DeepInformationPropagation,
  author           = {Schoenholz, Samuel S. and Gilmer, Justin and Ganguli, Surya and {Sohl-Dickstein}, Jascha},
  title            = {Deep {{Information Propagation}}},
  year             = {2017},
  month            = apr,
  abstract         = {We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively.},
  archiveprefix    = {arXiv},
  eprint           = {1611.01232},
  eprintclass      = {cs,stat},
  eprinttype       = {arxiv},
  file             = {:Schoenholz17 - Deep Information Propagation.pdf:PDF},
  keywords         = {Computer Science - Machine Learning, Statistics - Machine Learning},
  langid           = {english},
  modificationdate = {2022-04-02T17:57:15},
  primaryclass     = {cs, stat},
  priority         = {prio1},
}

@Article{Segadlo22UnifiedFieldTheory,
  author        = {Segadlo, Kai and Epping, Bastian and {van Meegen}, Alexander and Dahmen, David and Kr{\"a}mer, Michael and Helias, Moritz},
  title         = {Unified {{Field Theory}} for {{Deep}} and {{Recurrent Neural Networks}}},
  year          = {2022},
  month         = jan,
  abstract      = {Understanding capabilities and limitations of different network architectures is of fundamental importance to machine learning. Bayesian inference on Gaussian processes has proven to be a viable approach for studying recurrent and deep networks in the limit of infinite layer width, \$n\textbackslash to\textbackslash infty\$. Here we present a unified and systematic derivation of the mean-field theory for both architectures that starts from first principles by employing established methods from statistical physics of disordered systems. The theory elucidates that while the mean-field equations are different with regard to their temporal structure, they yet yield identical Gaussian kernels when readouts are taken at a single time point or layer, respectively. Bayesian inference applied to classification then predicts identical performance and capabilities for the two architectures. Numerically, we find that convergence towards the mean-field theory is typically slower for recurrent networks than for deep networks and the convergence speed depends non-trivially on the parameters of the weight prior as well as the depth or number of time steps, respectively. Our method exposes that Gaussian processes are but the lowest order of a systematic expansion in \$1/n\$. The formalism thus paves the way to investigate the fundamental differences between recurrent and deep architectures at finite widths \$n\$.},
  archiveprefix = {arXiv},
  eprint        = {2112.05589},
  eprintclass   = {cond-mat,stat},
  eprinttype    = {arxiv},
  file          = {:Segadlo22 - Unified Field Theory for Deep and Recurrent Neural Networks.pdf:PDF},
  keywords      = {Condensed Matter - Disordered Systems and Neural Networks, Statistics - Machine Learning},
  primaryclass  = {cond-mat, stat},
}

@Article{Seleznova20AnalyzingFiniteNeural,
  author  = {Seleznova, Mariia and Kutyniok, Gitta},
  journal = {ArXiv e-prints},
  title   = {Analyzing Finite Neural Networks: {{Can}} We Trust Neural Tangent Kernel Theory?},
  year    = {2020},
  eprint  = {2012.04477},
}

@Article{Smola04TutorialSupportVector,
  author   = {Smola, Alex J. and Sch{\"o}lkopf, Bernhard},
  journal  = {Statistics and Computing},
  title    = {A Tutorial on Support Vector Regression},
  year     = {2004},
  issn     = {0960-3174},
  month    = aug,
  number   = {3},
  pages    = {199--222},
  volume   = {14},
  abstract = {In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.},
  doi      = {10.1023/B:STCO.0000035301.49549.88},
  file     = {:Smola04 - A Tutorial on Support Vector Regression.pdf:PDF},
  langid   = {english},
}

@Article{SohlDickstein15DeepUnsupervisedLearning,
  author        = {{Sohl-Dickstein}, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  title         = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  year          = {2015},
  month         = nov,
  abstract      = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  archiveprefix = {arXiv},
  eprint        = {1503.03585},
  eprintclass   = {cond-mat,q-bio,stat},
  eprinttype    = {arxiv},
  file          = {:Sohl-Dickstein15 - Deep Unsupervised Learning Using Nonequilibrium Thermodynamics.pdf:;:Sohl-Dickstein15 - Deep Unsupervised Learning Using Nonequilibrium Thermodynamics.html:},
  keywords      = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
  primaryclass  = {cond-mat, q-bio, stat},
}

@Article{Stringer19HighDimensionalGeometry,
  author    = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Carandini, Matteo and Harris, Kenneth D},
  journal   = {Nature},
  title     = {High-Dimensional Geometry of Population Responses in Visual Cortex},
  year      = {2019},
  number    = {7765},
  pages     = {361--365},
  volume    = {571},
  file      = {:Stringer19 - High Dimensional Geometry of Population Responses in Visual Cortex.pdf:PDF},
  keywords  = {_tablet_modified},
  publisher = {{Nature Publishing Group}},
}

@Article{Wardak22ExtendedAndersonCriticality,
  author        = {Wardak, Asem and Gong, Pulin},
  title         = {Extended {{Anderson Criticality}} in {{Heavy-Tailed Neural Networks}}},
  year          = {2022},
  month         = feb,
  abstract      = {We investigate the emergence of complex dynamics in networks with heavy-tailed connectivity by developing a non-Hermitian random matrix theory. We uncover the existence of an extended critical regime of spatially multifractal fluctuations between the quiescent and active phases. This multifractal critical phase combines features of localization and delocalization and differs from the edge of chaos in classical networks by the appearance of universal hallmarks of Anderson criticality over an extended region in phase space. We show that the rich nonlinear response properties of the extended critical regime can account for a variety of neural dynamics such as the diversity of timescales, providing a computational advantage for persistent classification in a reservoir setting.},
  archiveprefix = {arXiv},
  eprint        = {2202.05527},
  eprintclass   = {cond-mat},
  eprinttype    = {arxiv},
  file          = {:Wardak22 - Extended Anderson Criticality in Heavy Tailed Neural Networks.pdf:;:Wardak22 - Extended Anderson Criticality in Heavy Tailed Neural Networks.html:},
  keywords      = {Condensed Matter - Disordered Systems and Neural Networks},
  primaryclass  = {cond-mat},
}

@Article{Wenzel20HowGoodIs,
  author  = {Wenzel, Florian and Roth, Kevin and Veeling, Bastiaan S. and {\'S}wi{\k{a}}tkowski, Jakub and Tran, Linh and Mandt, Stephan and Snoek, Jasper and Salimans, Tim and Jenatton, Rodolphe and Nowozin, Sebastian},
  journal = {ArXiv e-prints},
  title   = {How Good Is the Bayes Posterior in Deep Neural Networks Really?},
  year    = {2020},
  eprint  = {2002.02405},
}

@Article{Williams98ComputationInfiniteNeural,
  author    = {Williams, Christopher K. I.},
  journal   = {MIT Press Journals},
  title     = {Computation with Infinite Neural Networks},
  year      = {1998},
  publisher = {{MIT Press 238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu}},
}

@InProceedings{Xiao20DisentanglingTrainabilityGeneralization,
  author       = {Xiao, Lechao and Pennington, Jeffrey and Schoenholz, Samuel},
  booktitle    = {International Conference on Machine Learning},
  title        = {Disentangling Trainability and Generalization in Deep Neural Networks},
  year         = {2020},
  organization = {{PMLR}},
  pages        = {10462--10472},
  file         = {:Xiao20 - Disentangling Trainability and Generalization in Deep Neural Networks.pdf:PDF},
  keywords     = {_tablet_modified},
}

@Article{Yaida19NonGaussianProcesses,
  author  = {Yaida, Sho},
  journal = {ArXiv e-prints},
  title   = {Non-{{Gaussian}} Processes and Neural Networks at Finite Widths},
  year    = {2019},
  eprint  = {1910.00019},
}

@Article{Yang19ScalingLimitsWide,
  author  = {Yang, Greg},
  journal = {ArXiv e-prints},
  title   = {Scaling Limits of Wide Neural Networks with Weight Sharing: {{Gaussian}} Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation},
  year    = {2019},
  month   = feb,
  eprint  = {1902.04760},
}

@Article{Yang20TensorProgramsII,
  author  = {Yang, Greg},
  journal = {ArXiv e-prints},
  title   = {Tensor Programs {{II}}: {{Neural}} Tangent Kernel for Any Architecture},
  year    = {2020},
  eprint  = {2006.14548},
}

@Article{ZavatoneVeth21ExactPriorsFinite,
  author  = {{Zavatone-Veth}, Jacob A. and Pehlevan, Cengiz},
  journal = {ArXiv e-prints},
  title   = {Exact Priors of Finite Neural Networks},
  year    = {2021},
  eprint  = {2104.11734},
}

@Article{Cohen21Learningcurvesoverparametrized,
  author           = {Omry Cohen and Or Malka and Zohar Ringel},
  journal          = {Physical Review Research},
  title            = {Learning curves for overparametrized deep neural networks: A field theory perspective},
  year             = {2021},
  month            = {apr},
  number           = {2},
  pages            = {023034},
  volume           = {3},
  abstract         = {Phys. Rev. Res. 3, 023034 (2021). doi:10.1103/PhysRevResearch.3.023034},
  doi              = {10.1103/physrevresearch.3.023034},
  file             = {:Cohen21 - Learning Curves for Overparametrized Deep Neural Networks_ a Field Theory Perspective.pdf:PDF},
  keywords         = {doi:10.1103/PhysRevResearch.3.023034 url:https://doi.org/10.1103/PhysRevResearch.3.023034},
  modificationdate = {2022-04-02T15:02:08},
  priority         = {prio1},
  publisher        = {American Physical Society ({APS})},
}

@InProceedings{Helias05Globalhierarchyvs.,
  author = {Moritz Helias},
  title  = {Global hierarchy vs. local structure: spurious self-feedback in scale-free networks},
  year   = {2105},
  file   = {:Helias05 - Global Hierarchy Vs. Local Structure_ Spurious Self Feedback in Scale Free Networks.pdf:PDF},
}

@Article{Millidge21PredictiveCodingTheoretical,
  author        = {Beren Millidge and Anil Seth and Christopher L Buckley},
  title         = {Predictive Coding: a Theoretical and Experimental Review},
  year          = {2021},
  month         = jul,
  abstract      = {Predictive coding offers a potentially unifying account of cortical function -- postulating that the core function of the brain is to minimize prediction errors with respect to a generative model of the world. The theory is closely related to the Bayesian brain framework and, over the last two decades, has gained substantial influence in the fields of theoretical and cognitive neuroscience. A large body of research has arisen based on both empirically testing improved and extended theoretical and mathematical models of predictive coding, as well as in evaluating their potential biological plausibility for implementation in the brain and the concrete neurophysiological and psychological predictions made by the theory. Despite this enduring popularity, however, no comprehensive review of predictive coding theory, and especially of recent developments in this field, exists. Here, we provide a comprehensive review both of the core mathematical structure and logic of predictive coding, thus complementing recent tutorials in the literature. We also review a wide range of classic and recent work within the framework, ranging from the neurobiologically realistic microcircuits that could implement predictive coding, to the close relationship between predictive coding and the widely-used backpropagation of error algorithm, as well as surveying the close relationships between predictive coding and modern machine learning techniques.},
  archiveprefix = {arXiv},
  eprint        = {2107.12979},
  file          = {:Millidge21 - Predictive Coding_ a Theoretical and Experimental Review.pdf:PDF},
  keywords      = {cs.AI, cs.NE, q-bio.NC},
  primaryclass  = {cs.AI},
}

@Article{Meegen20LargeDeviationsApproach,
  author        = {Alexander van Meegen and Tobias Kühn and Moritz Helias},
  journal       = {Phys. Rev. Lett. 127, 158302 (2021)},
  title         = {Large Deviations Approach to Random Recurrent Neuronal Networks: Parameter Inference and Fluctuation-Induced Transitions},
  year          = {2020},
  month         = sep,
  abstract      = {We here unify the field theoretical approach to neuronal networks with large deviations theory. For a prototypical random recurrent network model with continuous-valued units, we show that the effective action is identical to the rate function and derive the latter using field theory. This rate function takes the form of a Kullback-Leibler divergence which enables data-driven inference of model parameters and calculation of fluctuations beyond mean-field theory. Lastly, we expose a regime with fluctuation-induced transitions between mean-field solutions.},
  archiveprefix = {arXiv},
  doi           = {10.1103/PhysRevLett.127.158302},
  eprint        = {2009.08889},
  file          = {:Meegen20 - Large Deviations Approach to Random Recurrent Neuronal Networks_ Parameter Inference and Fluctuation Induced Transitions.pdf:PDF},
  keywords      = {cond-mat.dis-nn, q-bio.NC},
  primaryclass  = {cond-mat.dis-nn},
}

@Book{Bianconi21HIGHERORDERNETWORKS,
  author    = {Bianconi, Ginestra},
  publisher = {Cambridge University Press},
  title     = {HIGHER ORDER NETWORKS: An Introduction to Simplicial Complexes},
  year      = {2021},
  file      = {:Bianconi21 - HIGHER ORDER NETWORKS_ an Introduction to Simplicial Complexes.pdf:PDF},
  groups    = {RNFlows JC},
}

@Article{Advani20Highdimensionaldynamics,
  author    = {Madhu S. Advani and Andrew M. Saxe and Haim Sompolinsky},
  journal   = {Neural Networks},
  title     = {High-dimensional dynamics of generalization error in neural networks},
  year      = {2020},
  month     = {dec},
  pages     = {428--446},
  volume    = {132},
  doi       = {10.1016/j.neunet.2020.08.022},
  file      = {:Advani20 - High Dimensional Dynamics of Generalization Error in Neural Networks.pdf:PDF},
  groups    = {RNFlows JC},
  publisher = {Elsevier {BV}},
}

@Article{Baldazzi21Essentialrenormalisationgroup,
  author        = {Alessio Baldazzi and Riccardo Ben Alì Zinati and Kevin Falls},
  title         = {Essential renormalisation group},
  year          = {2021},
  month         = may,
  abstract      = {We propose a novel scheme for the exact renormalisation group motivated by the desire of reducing the complexity of practical computations. The key idea is to specify renormalisation conditions for all inessential couplings, leaving us with the task of computing only the flow of the essential ones. To achieve this aim, we utilise a renormalisation group equation for the effective average action which incorporates general non-linear field reparameterisations. A prominent feature of the scheme is that, apart from the renormalisation of the mass, the propagator evaluated at any constant value of the field maintains its unrenormalised form. Conceptually, the scheme provides a clearer picture of renormalisation itself since the redundant, non-physical content is automatically disregarded in favour of a description based only on quantities that enter expressions for physical observables. To exemplify the scheme's utility, we investigate the Wilson-Fisher fixed point in three dimensions at order two in the derivative expansion. In this case, the scheme removes all order $\partial^2$ operators apart from the canonical term. Further simplifications occur at higher orders in the derivative expansion. Although we concentrate on a minimal scheme that reduces the complexity of computations, we propose more general schemes where inessential couplings can be tuned to optimise a given approximation. We further discuss the applicability of the scheme to a broad range of physical theories.},
  archiveprefix = {arXiv},
  eprint        = {2105.11482},
  file          = {:Baldazzi21 - Essential Renormalisation Group.pdf:PDF},
  groups        = {RNFlows JC},
  keywords      = {hep-th, cond-mat.stat-mech},
  primaryclass  = {hep-th},
}

@Article{Saunders02StringkernelsFisher,
  author  = {Saunders, Craig and Vinokourov, Alexei and Shawe-taylor, John},
  journal = {Advances in Neural Information Processing Systems},
  title   = {String kernels, Fisher kernels and finite state automata},
  year    = {2002},
  volume  = {15},
  file    = {:Saunders02 - String Kernels, Fisher Kernels and Finite State Automata.pdf:PDF},
}

@Article{Arute19QuantumSupremacyUsing,
  author  = {Arute, Frank and Arya, Kunal and Babbush, Ryan and Bacon, Dave and Bardin, Joseph C. and Barends, Rami and Biswas, Rupak and Boixo, Sergio and Brandao, Fernando G. S. L. and Buell, David A. and Burkett, Brian and Chen, Yu and Chen, Zijun and Chiaro, Ben and Collins, Roberto and Courtney, William and Dunsworth, Andrew and Farhi, Edward and Foxen, Brooks and Fowler, Austin and Gidney, Craig and Giustina, Marissa and Graff, Rob and Guerin, Keith and Habegger, Steve and Harrigan, Matthew P. and Hartmann, Michael J. and Ho, Alan and Hoffmann, Markus and Huang, Trent and Humble, Travis S. and Isakov, Sergei V. and Jeffrey, Evan and Jiang, Zhang and Kafri, Dvir and Kechedzhi, Kostyantyn and Kelly, Julian and Klimov, Paul V. and Knysh, Sergey and Korotkov, Alexander and Kostritsa, Fedor and Landhuis, David and Lindmark, Mike and Lucero, Erik and Lyakh, Dmitry and Mandr{\`a}, Salvatore and McClean, Jarrod R. and McEwen, Matthew and Megrant, Anthony and Mi, Xiao and Michielsen, Kristel and Mohseni, Masoud and Mutus, Josh and Naaman, Ofer and Neeley, Matthew and Neill, Charles and Niu, Murphy Yuezhen and Ostby, Eric and Petukhov, Andre and Platt, John C. and Quintana, Chris and Rieffel, Eleanor G. and Roushan, Pedram and Rubin, Nicholas C. and Sank, Daniel and Satzinger, Kevin J. and Smelyanskiy, Vadim and Sung, Kevin J. and Trevithick, Matthew D. and Vainsencher, Amit and Villalonga, Benjamin and White, Theodore and Yao, Z. Jamie and Yeh, Ping and Zalcman, Adam and Neven, Hartmut and Martinis, John M.},
  journal = {Nature},
  title   = {Quantum Supremacy Using a Programmable Superconducting Processor},
  year    = {2019},
  issn    = {0028-0836, 1476-4687},
  month   = oct,
  number  = {7779},
  pages   = {505--510},
  volume  = {574},
  doi     = {10.1038/s41586-019-1666-5},
  file    = {:Arute19 - Quantum Supremacy Using a Programmable Superconducting Processor.pdf:PDF},
  groups  = {R32 Bosch},
  langid  = {english},
}

@Article{BauerBayesianContextAggregation,
  author = {Bauer, Jan},
  title  = {Bayesian {{Context Aggregation}}},
  pages  = {14},
  file   = {:Bauer - Bayesian Context Aggregation.pdf:PDF},
  groups = {R32 Bosch},
  langid = {english},
}

@Article{BauerConvolutionalNeuralProcesses,
  author = {Bauer, Jan},
  title  = {Convolutional {{Neural Processes}}},
  pages  = {24},
  file   = {:Bauer - Convolutional Neural Processes.pdf:PDF},
  groups = {R32 Bosch},
  langid = {english},
}

@Article{BauerNeuralProcessesBayesian,
  author  = {Bauer, Jan},
  journal = {oz ..},
  title   = {Neural {{Processes}} with {{Bayesian Aggregation}}},
  pages   = {44},
  groups  = {R32 Bosch},
  langid  = {english},
}

@Article{Bronstein21GeometricDeepLearning,
  author        = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veli{\v c}kovi{\'c}, Petar},
  title         = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  year          = {2021},
  month         = may,
  abstract      = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  archiveprefix = {arXiv},
  eprint        = {2104.13478},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Artificial Intelligence, Computer Science - Computational Geometry, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
  shorttitle    = {Geometric {{Deep Learning}}},
}

@Article{Bruinsma21GaussianNeuralProcess,
  author        = {Bruinsma, Wessel P. and Requeima, James and Foong, Andrew Y. K. and Gordon, Jonathan and Turner, Richard E.},
  title         = {The {{Gaussian Neural Process}}},
  year          = {2021},
  month         = jan,
  abstract      = {Neural Processes (NPs; Garnelo et al., 2018a,b) are a rich class of models for meta-learning that map data sets directly to predictive stochastic processes. We provide a rigorous analysis of the standard maximum-likelihood objective used to train conditional NPs. Moreover, we propose a new member to the Neural Process family called the Gaussian Neural Process (GNP), which models predictive correlations, incorporates translation equivariance, provides universal approximation guarantees, and demonstrates encouraging performance.},
  archiveprefix = {arXiv},
  eprint        = {2101.03606},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  file          = {:Bruinsma21 - The Gaussian Neural Process.pdf:PDF},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
}

@Article{Burda16ImportanceWeightedAutoencoders,
  author        = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  title         = {Importance {{Weighted Autoencoders}}},
  year          = {2016},
  month         = nov,
  abstract      = {The variational autoencoder (VAE; Kingma \& Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  archiveprefix = {arXiv},
  eprint        = {1509.00519},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  file          = {:Burda16 - Importance Weighted Autoencoders.pdf:PDF},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
}

@Article{CohenGroupEquivariantConvolutional,
  author   = {Cohen, Taco S and Cohen, T S and Nl, Uva},
  title    = {Group {{Equivariant Convolutional Networks}}},
  pages    = {10},
  abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.},
  file     = {:Cohen - Group Equivariant Convolutional Networks.pdf:PDF},
  groups   = {R32 Bosch},
  langid   = {english},
}

@Article{Ebden15GaussianProcessesQuick,
  author        = {Ebden, Mark},
  title         = {Gaussian {{Processes}}: {{A Quick Introduction}}},
  year          = {2015},
  month         = aug,
  abstract      = {A gentle introduction to Gaussian processes (GPs). The three parts of the document consider GPs for regression, classification, and dimensionality reduction.},
  archiveprefix = {arXiv},
  eprint        = {1505.02965},
  eprintclass   = {math,stat},
  eprinttype    = {arxiv},
  groups        = {R32 Bosch},
  keywords      = {Mathematics - Statistics Theory},
  langid        = {english},
  primaryclass  = {math, stat},
  shorttitle    = {Gaussian {{Processes}}},
}

@Article{EmmertStreib20IntroductoryReviewDeep,
  author   = {{Emmert-Streib}, Frank and Yang, Zhen and Feng, Han and Tripathi, Shailesh and Dehmer, Matthias},
  journal  = {Frontiers in Artificial Intelligence},
  title    = {An {{Introductory Review}} of {{Deep Learning}} for {{Prediction Models With Big Data}}},
  year     = {2020},
  issn     = {2624-8212},
  month    = feb,
  pages    = {4},
  volume   = {3},
  abstract = {Deep learning models stand for a new learning paradigm in artificial intelligence (AI) and machine learning. Recent breakthrough results in image analysis and speech recognition have generated a massive interest in this field because also applications in many other domains providing big data seem possible. On a downside, the mathematical and computational methodology underlying deep learning models is very challenging, especially for interdisciplinary scientists. For this reason, we present in this paper an introductory review of deep learning approaches including Deep Feedforward Neural Networks (D-FFNN), Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), Autoencoders (AEs), and Long Short-Term Memory (LSTM) networks. These models form the major core architectures of deep learning models currently used and should belong in any data scientist's toolbox. Importantly, those core architectural building blocks can be composed flexibly\textemdash in an almost Lego-like manner\textemdash to build new application-specific network architectures. Hence, a basic understanding of these network architectures is important to be prepared for future developments in AI.},
  doi      = {10.3389/frai.2020.00004},
  groups   = {R32 Bosch},
  langid   = {english},
}

@Article{Finn17ModelAgnosticMeta,
  author        = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  title         = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  year          = {2017},
  month         = jul,
  abstract      = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two fewshot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  archiveprefix = {arXiv},
  eprint        = {1703.03400},
  eprintclass   = {cs},
  eprinttype    = {arxiv},
  file          = {:Finn17 - Model Agnostic Meta Learning for Fast Adaptation of Deep Networks.pdf:PDF},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  langid        = {english},
  primaryclass  = {cs},
}

@Article{Finzi20GeneralizingConvolutionalNeural,
  author        = {Finzi, Marc and Stanton, Samuel and Izmailov, Pavel and Wilson, Andrew Gordon},
  title         = {Generalizing {{Convolutional Neural Networks}} for {{Equivariance}} to {{Lie Groups}} on {{Arbitrary Continuous Data}}},
  year          = {2020},
  month         = sep,
  abstract      = {The translation equivariance of convolutional layers enables convolutional neural networks to generalize well on image problems. While translation equivariance provides a powerful inductive bias for images, we often additionally desire equivariance to other transformations, such as rotations, especially for non-image data. We propose a general method to construct a convolutional layer that is equivariant to transformations from any specified Lie group with a surjective exponential map. Incorporating equivariance to a new group requires implementing only the group exponential and logarithm maps, enabling rapid prototyping. Showcasing the simplicity and generality of our method, we apply the same model architecture to images, ball-and-stick molecular data, and Hamiltonian dynamical systems. For Hamiltonian systems, the equivariance of our models is especially impactful, leading to exact conservation of linear and angular momentum.},
  archiveprefix = {arXiv},
  eprint        = {2002.12880},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
}

@Article{Foong20MetaLearningStationary,
  author        = {Foong, Andrew Y. K. and Bruinsma, Wessel P. and Gordon, Jonathan and Dubois, Yann and Requeima, James and Turner, Richard E.},
  title         = {Meta-{{Learning Stationary Stochastic Process Prediction}} with {{Convolutional Neural Processes}}},
  year          = {2020},
  month         = nov,
  abstract      = {Stationary stochastic processes (SPs) are a key component of many probabilistic models, such as those for off-the-grid spatio-temporal data. They enable the statistical symmetry of underlying physical phenomena to be leveraged, thereby aiding generalization. Prediction in such models can be viewed as a translation equivariant map from observed data sets to predictive SPs, emphasizing the intimate relationship between stationarity and equivariance. Building on this, we propose the Convolutional Neural Process (ConvNP), which endows Neural Processes (NPs) with translation equivariance and extends convolutional conditional NPs to allow for dependencies in the predictive distribution. The latter enables ConvNPs to be deployed in settings which require coherent samples, such as Thompson sampling or conditional image completion. Moreover, we propose a new maximum-likelihood objective to replace the standard ELBO objective in NPs, which conceptually simplifies the framework and empirically improves performance. We demonstrate the strong performance and generalization capabilities of ConvNPs on 1D regression, image completion, and various tasks with real-world spatio-temporal data.},
  archiveprefix = {arXiv},
  eprint        = {2007.01332},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
}

@Article{GarneloConditionalNeuralProcesses,
  author   = {Garnelo, Marta and Rosenbaum, Dan and Maddison, Chris J and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo J and Eslami, S M Ali},
  title    = {Conditional {{Neural Processes}}},
  pages    = {10},
  abstract = {Deep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exploit prior knowledge to quickly infer the shape of a new function at test time. Yet GPs are computationally expensive, and it can be hard to design appropriate priors. In this paper we propose a family of neural models, Conditional Neural Processes (CNPs), that combine the benefits of both. CNPs are inspired by the flexibility of stochastic processes such as GPs, but are structured as neural networks and trained via gradient descent. CNPs make accurate predictions after observing only a handful of training data points, yet scale to complex functions and large datasets. We demonstrate the performance and versatility of the approach on a range of canonical machine learning tasks, including regression, classification and image completion.},
  file     = {:Garnelo - Conditional Neural Processes.pdf:PDF},
  groups   = {R32 Bosch},
  langid   = {english},
}

@Article{Garnelo18NeuralProcesses,
  author        = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
  title         = {Neural {{Processes}}},
  year          = {2018},
  month         = jul,
  abstract      = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
  archiveprefix = {arXiv},
  eprint        = {1807.01622},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
}

@Article{Gordon20ConvoluationalConditionalNeural,
  author  = {Gordon, Jonathan and Bruinsma, Wessel P and Foong, Andrew Y K},
  journal = {International Conference on Learning Representations},
  title   = {Convoluational {{Conditional Neural Processes}}},
  year    = {2020},
  pages   = {32},
  groups  = {R32 Bosch},
  langid  = {english},
}

@Article{Gordon19MetaLearningProbabilistic,
  author        = {Gordon, Jonathan and Bronskill, John and Bauer, Matthias and Nowozin, Sebastian and Turner, Richard E.},
  title         = {Meta-{{Learning Probabilistic Inference For Prediction}}},
  year          = {2019},
  month         = aug,
  abstract      = {This paper introduces a new framework for data efficient and versatile learning. Specifically: 1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction. ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. 2) We introduce VERSA, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. VERSA substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training. 3) We evaluate VERSA on benchmark datasets where the method sets new state-of-the-art results, handles arbitrary numbers of shots, and for classification, arbitrary numbers of classes at train and test time. The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task.},
  archiveprefix = {arXiv},
  eprint        = {1805.09921},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
}

@Article{Kawano21GroupEquivariantConditional,
  author        = {Kawano, Makoto and Kumagai, Wataru and Sannai, Akiyoshi and Iwasawa, Yusuke and Matsuo, Yutaka},
  title         = {Group {{Equivariant Conditional Neural Processes}}},
  year          = {2021},
  month         = feb,
  abstract      = {We present the group equivariant conditional neural process (EquivCNP), a metalearning method with permutation invariance in a data set as in conventional conditional neural processes (CNPs), and it also has transformation equivariance in data space. Incorporating group equivariance, such as rotation and scaling equivariance, provides a way to consider the symmetry of real-world data. We give a decomposition theorem for permutation-invariant and group-equivariant maps, which leads us to construct EquivCNPs with an infinite-dimensional latent space to handle group symmetries. In this paper, we build architecture using Lie group convolutional layers for practical implementation. We show that EquivCNP with translation equivariance achieves comparable performance to conventional CNPs in a 1D regression task. Moreover, we demonstrate that incorporating an appropriate Lie group equivariance, EquivCNP is capable of zero-shot generalization for an image-completion task by selecting an appropriate Lie group equivariance.},
  archiveprefix = {arXiv},
  eprint        = {2102.08759},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
}

@Article{Kim19AttentiveNeuralProcesses,
  author        = {Kim, Hyunjik and Mnih, Andriy and Schwarz, Jonathan and Garnelo, Marta and Eslami, Ali and Rosenbaum, Dan and Vinyals, Oriol and Teh, Yee Whye},
  title         = {Attentive {{Neural Processes}}},
  year          = {2019},
  month         = jul,
  abstract      = {Neural Processes (NPs) (Garnelo et al., 2018a;b) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.},
  archiveprefix = {arXiv},
  eprint        = {1901.05761},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  file          = {:Kim19 - Attentive Neural Processes.pdf:PDF},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
}

@Article{Kingma17AdamMethodStochastic,
  author        = {Kingma, Diederik P. and Ba, Jimmy},
  title         = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  year          = {2017},
  month         = jan,
  abstract      = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  eprint        = {1412.6980},
  eprintclass   = {cs},
  eprinttype    = {arxiv},
  file          = {:Kingma17 - Adam_ a Method for Stochastic Optimization.pdf:PDF},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Machine Learning},
  langid        = {english},
  primaryclass  = {cs},
  shorttitle    = {Adam},
}

@Article{Kingma14AutoEncodingVariational,
  author        = {Kingma, Diederik P. and Welling, Max},
  title         = {Auto-{{Encoding Variational Bayes}}},
  year          = {2014},
  month         = may,
  abstract      = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  eprint        = {1312.6114},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  file          = {:Kingma14 - Auto Encoding Variational Bayes.pdf:PDF},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
}

@PhdThesis{Kipf20DeepLearningGraph,
  author = {Kipf, Thomas},
  title  = {Deep Learning with Graph-Structured Representations},
  year   = {2020},
  annote = {OCLC: 1151818405},
  groups = {R32 Bosch},
  isbn   = {9789463758512},
  langid = {english},
}

@Article{LeCunTutorialEnergyBased,
  author   = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc'Aurelio and Huang, Fu Jie},
  title    = {A {{Tutorial}} on {{Energy-Based Learning}}},
  pages    = {59},
  abstract = {Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variables. Inference consists in clamping the value of observed variables and finding configurations of the remaining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discriminative and generative approaches, as well as graph-transformer networks, conditional random fields, maximum margin Markov networks, and several manifold learning methods.},
  groups   = {R32 Bosch},
  langid   = {english},
}

@Article{Lee20BootstrappingNeuralProcesses,
  author        = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Yang, Eunho and Hwang, Sung Ju and Teh, Yee Whye},
  title         = {Bootstrapping {{Neural Processes}}},
  year          = {2020},
  month         = oct,
  abstract      = {Unlike in the traditional statistical modeling for which a user typically hand-specify a prior, Neural Processs (NPs) implicitly define a broad class of stochastic processes with neural networks. Given a data stream, NP learns a stochastic process that best describes the data. While this ``data-driven'' way of learning stochastic processes has proven to handle various types of data, NPs still rely on an assumption that uncertainty in stochastic processes is modeled by a single latent variable, which potentially limits the flexibility. To this end, we propose the Bootstrapping Neural Process (BNP), a novel extension of the NP family using the bootstrap. The bootstrap is a classical data-driven technique for estimating uncertainty, which allows BNP to learn the stochasticity in NPs without assuming a particular form. We demonstrate the efficacy of BNP on various types of data and its robustness in the presence of model-data mismatch.},
  archiveprefix = {arXiv},
  eprint        = {2008.02956},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  file          = {:Lee20 - Bootstrapping Neural Processes.pdf:PDF},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
}

@Article{LeEmpiricalEvaluationNeural,
  author   = {Le, Tuan Anh and Kim, Hyunjik and Garnelo, Marta},
  title    = {Empirical {{Evaluation}} of {{Neural Process Objectives}}},
  pages    = {9},
  abstract = {Neural processes (NPs) [3, 4] are parametric stochastic processes that can be trained from a dataset consisting of sets of input-output pairs. During test time, given a context set of input-output pairs and a set of target inputs, they allow us to approximate the posterior predictive of the target outputs. NPs have shown promise in applications such as image super-resolution, conditional image generation or scalable Bayesian optimization. It is, however, unclear which objective and model specification should be used to train NPs. This abstract empirically evaluates the performance of NPs for different objectives and model specifications. Given that some objectives and model specifications clearly outperform others, our analysis can be useful in guiding future research and applications of NPs.},
  file     = {:Le - Empirical Evaluation of Neural Process Objectives.pdf:PDF},
  groups   = {R32 Bosch},
  langid   = {english},
}

@Article{Louizos19FunctionalNeuralProcess,
  author        = {Louizos, Christos and Shi, Xiahan and Schutte, Klamer and Welling, Max},
  title         = {The {{Functional Neural Process}}},
  year          = {2019},
  month         = nov,
  abstract      = {We present a new family of exchangeable stochastic processes, the Functional Neural Processes (FNPs). FNPs model distributions over functions by learning a graph of dependencies on top of latent representations of the points in the given dataset. In doing so, they define a Bayesian model without explicitly positing a prior distribution over latent global parameters; they instead adopt priors over the relational structure of the given dataset, a task that is much simpler. We show how we can learn such models from data, demonstrate that they are scalable to large datasets through mini-batch optimization and describe how we can make predictions for new points via their posterior predictive distribution. We experimentally evaluate FNPs on the tasks of toy regression and image classification and show that, when compared to baselines that employ global latent parameters, they offer both competitive predictions as well as more robust uncertainty estimates.},
  archiveprefix = {arXiv},
  eprint        = {1906.08324},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  file          = {:Louizos19 - The Functional Neural Process.pdf:PDF},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
}

@Article{Mirhoseini21GraphPlacementMethodology,
  author  = {Mirhoseini, Azalia and Goldie, Anna and Yazgan, Mustafa and Jiang, Joe Wenjie and Songhori, Ebrahim and Wang, Shen and Lee, Young-Joon and Johnson, Eric and Pathak, Omkar and Nazi, Azade and Pak, Jiwoo and Tong, Andy and Srinivasa, Kavya and Hang, William and Tuncer, Emre and Le, Quoc V. and Laudon, James and Ho, Richard and Carpenter, Roger and Dean, Jeff},
  journal = {Nature},
  title   = {A Graph Placement Methodology for Fast Chip Design},
  year    = {2021},
  issn    = {0028-0836, 1476-4687},
  month   = jun,
  number  = {7862},
  pages   = {207--212},
  volume  = {594},
  doi     = {10.1038/s41586-021-03544-w},
  groups  = {R32 Bosch},
  langid  = {english},
}

@Article{Naderiparizi20UncertaintyNeuralProcesses,
  author        = {Naderiparizi, Saeid and Chiu, Kenny and {Bloem-Reddy}, Benjamin and Wood, Frank},
  title         = {Uncertainty in {{Neural Processes}}},
  year          = {2020},
  month         = oct,
  abstract      = {We explore the effects of architecture and training objective choice on amortized posterior predictive inference in probabilistic conditional generative models. We aim this work to be a counterpoint to a recent trend in the literature that stresses achieving good samples when the amount of conditioning data is large. We instead focus our attention on the case where the amount of conditioning data is small. We highlight specific architecture and objective choices that we find lead to qualitative and quantitative improvement to posterior inference in this low data regime. Specifically we explore the effects of choices of pooling operator and variational family on posterior quality in neural processes. Superior posterior predictive samples drawn from our novel neural process architectures are demonstrated via image completion/in-painting experiments.},
  archiveprefix = {arXiv},
  eprint        = {2010.03753},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  file          = {:Naderiparizi20 - Uncertainty in Neural Processes.pdf:PDF},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
}

@Article{Otto21DIFFERENTIABLETRUSTREGION,
  author   = {Otto, Fabian and Becker, Philipp and Vien, Ngo Anh and Ziesche, Hanna Carolin and Neumann, Gerhard},
  title    = {{{DIFFERENTIABLE TRUST REGION LAYERS FOR DEEP REINFORCEMENT LEARNING}}},
  year     = {2021},
  pages    = {22},
  abstract = {Trust region methods are a popular tool in reinforcement learning as they yield robust policy updates in continuous and discrete action spaces. However, enforcing such trust regions in deep reinforcement learning is difficult. Hence, many approaches, such as Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), are based on approximations. Due to those approximations, they violate the constraints or fail to find the optimal solution within the trust region. Moreover, they are difficult to implement, often lack sufficient exploration, and have been shown to depend on seemingly unrelated implementation choices. In this work, we propose differentiable neural network layers to enforce trust regions for deep Gaussian policies via closed-form projections. Unlike existing methods, those layers formalize trust regions for each state individually and can complement existing reinforcement learning algorithms. We derive trust region projections based on the Kullback-Leibler divergence, the Wasserstein L2 distance, and the Frobenius norm for Gaussian distributions. We empirically demonstrate that those projection layers achieve similar or better results than existing methods while being almost agnostic to specific implementation choices. The code is available at https://git.io/Jthb0.},
  file     = {:Otto21 - DIFFERENTIABLE TRUST REGION LAYERS fOR DEEP REINFORCEMENT LEARNING.pdf:PDF},
  groups   = {R32 Bosch},
  langid   = {english},
}

@Article{Pearl09CausalInferenceStatistics,
  author     = {Pearl, Judea},
  journal    = {Statistics Surveys},
  title      = {Causal Inference in Statistics: {{An}} Overview},
  year       = {2009},
  issn       = {1935-7516},
  month      = jan,
  number     = {none},
  volume     = {3},
  abstract   = {This review presents empirical researchers with recent advances in causal inference, and stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underly all causal inferences, the languages used in formulating those assumptions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and unifies other approaches to causation, and provides a coherent mathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: (1) queries about the effects of potential interventions, (also called ``causal effects'' or ``policy evaluation'') (2) queries about probabilities of counterfactuals, (including assessment of ``regret,'' ``attribution'' or ``causes of effects'') and (3) queries about direct and indirect effects (also known as ``mediation''). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both.},
  doi        = {10.1214/09-SS057},
  groups     = {R32 Bosch},
  langid     = {english},
  shorttitle = {Causal Inference in Statistics},
}

@Article{PearlREASONINGCAUSEEFFECT,
  author = {Pearl, Judea},
  title  = {{{REASONING WITH CAUSE AND EFFECT}}},
  pages  = {83},
  file   = {:Pearl - REASONING wITH CAUSE aND EFFECT.pdf:PDF},
  groups = {R32 Bosch},
  langid = {english},
}

@Article{Qiu21NeuralTransformationLearning,
  author        = {Qiu, Chen and Pfrommer, Timo and Kloft, Marius and Mandt, Stephan and Rudolph, Maja},
  title         = {Neural {{Transformation Learning}} for {{Deep Anomaly Detection Beyond Images}}},
  year          = {2021},
  month         = jul,
  abstract      = {Data transformations (e.g. rotations, reflections, and cropping) play an important role in selfsupervised learning. Typically, images are transformed into different views, and neural networks trained on tasks involving these views produce useful feature representations for downstream tasks, including anomaly detection. However, for anomaly detection beyond image data, it is often unclear which transformations to use. Here we present a simple end-to-end procedure for anomaly detection with learnable transformations. The key idea is to embed the transformed data into a semantic space such that the transformed data still resemble their untransformed form, while different transformations are easily distinguishable. Extensive experiments on time series demonstrate that we significantly outperform existing methods on the one-vs.-rest setting but also on the more challenging n-vs.-rest anomaly-detection task. On tabular datasets from the medical and cybersecurity domains, our method learns domainspecific transformations and detects anomalies more accurately than previous work.},
  archiveprefix = {arXiv},
  eprint        = {2103.16440},
  eprintclass   = {cs},
  eprinttype    = {arxiv},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  langid        = {english},
  primaryclass  = {cs},
}

@Book{Rasmussen06GaussianProcessesMachinea,
  author    = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  publisher = {{MIT Press}},
  title     = {Gaussian Processes for Machine Learning},
  year      = {2006},
  address   = {{Cambridge, Mass}},
  isbn      = {978-0-262-18253-9},
  series    = {Adaptive Computation and Machine Learning},
  annote    = {OCLC: ocm61285753},
  file      = {:Rasmussen und Williams - 2006 - Gaussian processes for machine learning.pdf:PDF},
  groups    = {R32 Bosch},
  keywords  = {Data processing, Gaussian processes, Machine learning, Mathematical models},
  langid    = {english},
  lccn      = {QA274.4 .R37 2006},
}

@Article{Ravi17OPTIMIZATIONASMODEL,
  author   = {Ravi, Sachin and Larochelle, Hugo},
  title    = {{{OPTIMIZATION AS A MODEL FOR FEW-SHOT LEARNING}}},
  year     = {2017},
  pages    = {11},
  abstract = {Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a classifier has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity classifiers requires many iterative steps over many examples to perform well. Here, we propose an LSTMbased meta-learner model to learn the exact optimization algorithm used to train another learner neural network classifier in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner (classifier) network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning.},
  groups   = {R32 Bosch},
  langid   = {english},
}

@Article{RezendeStochasticBackpropagationApproximate,
  author   = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  title    = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  pages    = {9},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation \textendash{} rules for gradient backpropagation through stochastic variables \textendash and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.},
  file     = {:Rezende - Stochastic Backpropagation and Approximate Inference in Deep Generative Models.pdf:PDF},
  groups   = {R32 Bosch},
  langid   = {english},
}

@Article{Sandryhaila13DiscreteSignalProcessing,
  author   = {Sandryhaila, Aliaksei and Moura, Jos{\'e} M. F.},
  journal  = {IEEE Transactions on Signal Processing},
  title    = {Discrete {{Signal Processing}} on {{Graphs}}},
  year     = {2013},
  issn     = {1053-587X, 1941-0476},
  month    = apr,
  number   = {7},
  pages    = {1644--1656},
  volume   = {61},
  abstract = {In social settings, individuals interact through webs of relationships. Each individual is a node in a complex network (or graph) of interdependencies and generates data, lots of data. We label the data by its source, or formally stated, we index the data by the nodes of the graph. The resulting signals (data indexed by the nodes) are far removed from time or image signals indexed by well ordered time samples or pixels. DSP, discrete signal processing, provides a comprehensive, elegant, and efficient methodology to describe, represent, transform, analyze, process, or synthesize these well ordered time or image signals. This paper extends to signals on graphs DSP and its basic tenets, including filters, convolution, -transform, impulse response, spectral representation, Fourier transform, frequency response, and illustrates DSP on graphs by classifying blogs, linear predicting and compressing data from irregularly located weather stations, or predicting behavior of customers of a mobile service provider.},
  doi      = {10.1109/TSP.2013.2238935},
  file     = {:Sandryhaila13 - Discrete Signal Processing on Graphs.pdf:PDF},
  groups   = {R32 Bosch},
  langid   = {english},
}

@Article{Silver16MasteringGameGo,
  author  = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  journal = {Nature},
  title   = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  year    = {2016},
  issn    = {0028-0836, 1476-4687},
  month   = jan,
  number  = {7587},
  pages   = {484--489},
  volume  = {529},
  doi     = {10.1038/nature16961},
  groups  = {R32 Bosch},
  langid  = {english},
}

@Article{Silver17MasteringGameGo,
  author           = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  journal          = {Nature},
  title            = {Mastering the Game of {{Go}} without Human Knowledge},
  year             = {2017},
  issn             = {0028-0836, 1476-4687},
  month            = oct,
  number           = {7676},
  pages            = {354--359},
  volume           = {550},
  doi              = {10.1038/nature24270},
  file             = {:Silver17 - Mastering the Game of Go without Human Knowledge.pdf:PDF},
  groups           = {R32 Bosch},
  langid           = {english},
  modificationdate = {2022-07-12T11:31:53},
}

@Article{SohnLearningStructuredOutput,
  author   = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  title    = {Learning {{Structured Output Representation}} Using {{Deep Conditional Generative Models}}},
  pages    = {9},
  abstract = {Supervised deep learning has been successfully applied to many recognition problems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows for fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
  file     = {:Sohn - Learning Structured Output Representation Using Deep Conditional Generative Models.pdf:PDF},
  groups   = {R32 Bosch},
  langid   = {english},
}

@Article{Vahdat21NVAEDeepHierarchical,
  author        = {Vahdat, Arash and Kautz, Jan},
  title         = {{{NVAE}}: {{A Deep Hierarchical Variational Autoencoder}}},
  year          = {2021},
  month         = jan,
  abstract      = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ as shown in Fig. 1. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\texttimes 256 pixels. The source code is available at https://github.com/NVlabs/NVAE.},
  archiveprefix = {arXiv},
  eprint        = {2007.03898},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
  shorttitle    = {{{NVAE}}},
}

@Article{Vaswani17AttentionIsAll,
  author        = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  title         = {Attention {{Is All You Need}}},
  year          = {2017},
  month         = dec,
  abstract      = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  eprint        = {1706.03762},
  eprintclass   = {cs},
  eprinttype    = {arxiv},
  file          = {:Vaswani17 - Attention Is All You Need.pdf:PDF},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  langid        = {english},
  primaryclass  = {cs},
}

@Article{VolppDataAugmentationNeural,
  author = {Volpp, Michael and Vashishtha, Anushka},
  title  = {Data {{Augmentation}} for {{Neural Processes}}},
  pages  = {24},
  groups = {R32 Bosch},
  langid = {english},
}

@Article{Volpp20MetaLearningAcquisition,
  author        = {Volpp, Michael and Fr{\"o}hlich, Lukas P. and Fischer, Kirsten and Doerr, Andreas and Falkner, Stefan and Hutter, Frank and Daniel, Christian},
  title         = {Meta-{{Learning Acquisition Functions}} for {{Transfer Learning}} in {{Bayesian Optimization}}},
  year          = {2020},
  month         = feb,
  abstract      = {Transferring knowledge across tasks to improve data-efficiency is one of the open key challenges in the field of global black-box optimization. Readily available algorithms are typically designed to be universal optimizers and, therefore, often suboptimal for specific tasks. We propose a novel transfer learning method to obtain customized optimizers within the well-established framework of Bayesian optimization, allowing our algorithm to utilize the proven generalization capabilities of Gaussian processes. Using reinforcement learning to meta-train an acquisition function (AF) on a set of related tasks, the proposed method learns to extract implicit structural information and to exploit it for improved data-efficiency. We present experiments on a simulation-to-real transfer task as well as on several synthetic functions and on two hyperparameter search problems. The results show that our algorithm (1) automatically identifies structural properties of objective functions from available source tasks or simulations, (2) performs favourably in settings with both scarse and abundant source data, and (3) falls back to the performance level of general AFs if no particular structure is present.},
  archiveprefix = {arXiv},
  eprint        = {1904.02642},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  file          = {:Volpp20 - Meta Learning Acquisition Functions for Transfer Learning in Bayesian Optimization.pdf:PDF},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
}

@Article{Wang20DoublyStochasticVariational,
  author        = {Wang, Qi and {van Hoof}, Herke},
  title         = {Doubly {{Stochastic Variational Inference}} for {{Neural Processes}} with {{Hierarchical Latent Variables}}},
  year          = {2020},
  month         = oct,
  abstract      = {Neural processes (NPs) constitute a family of variational approximate models for stochastic processes with promising properties in computational efficiency and uncertainty quantification. These processes use neural networks with latent variable inputs to induce predictive distributions. However, the expressiveness of vanilla NPs is limited as they only use a global latent variable, while targetspecific local variation may be crucial sometimes. To address this challenge, we investigate NPs systematically and present a new variant of NP model that we call Doubly Stochastic Variational Neural Process (DSVNP). This model combines the global latent variable and local latent variables for prediction. We evaluate this model in several experiments, and our results demonstrate competitive prediction performance in multi-output regression and uncertainty estimation in classification.},
  archiveprefix = {arXiv},
  eprint        = {2008.09469},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Machine Learning, Statistics - Applications, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
}

@Article{Xu21HowNeuralNetworks,
  author        = {Xu, Keyulu and Zhang, Mozhi and Li, Jingling and Du, Simon S. and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
  title         = {How {{Neural Networks Extrapolate}}: {{From Feedforward}} to {{Graph Neural Networks}}},
  year          = {2021},
  month         = mar,
  abstract      = {We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), do not extrapolate well in certain simple tasks, Graph Neural Networks (GNNs) \textendash{} structured networks with MLP modules \textendash{} have shown some success in more complex tasks. Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most nonlinear functions. But, they can provably learn a linear target function when the training distribution is sufficiently ``diverse''. Second, in connection to analyzing the successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-specific non-linearities in the architecture or features. Our theoretical analysis builds on a connection of over-parameterized networks to the neural tangent kernel. Empirically, our theory holds across different training settings.},
  archiveprefix = {arXiv},
  eprint        = {2009.11848},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  file          = {:Xu21 - How Neural Networks Extrapolate_ from Feedforward to Graph Neural Networks.pdf:PDF},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
  shorttitle    = {How {{Neural Networks Extrapolate}}},
}

@Article{Yin21AUGMENTINGPHYSICALMODELS,
  author   = {Yin, Yuan and Guen, Vincent Le and Dona, J{\'e}r{\'e}mie and {de B{\'e}zenac}, Emmanuel and Ayed, Ibrahim and Thome, Nicolas and Gallinari, Patrick},
  title    = {{{AUGMENTING PHYSICAL MODELS WITH DEEP NET- WORKS FOR COMPLEX DYNAMICS FORECASTING}}},
  year     = {2021},
  pages    = {22},
  abstract = {Forecasting complex dynamical phenomena in settings where only partial knowledge of their dynamics is available is a prevalent problem across various scientific fields. While purely data-driven approaches are arguably insufficient in this context, standard physical modeling based approaches tend to be over-simplistic, inducing non-negligible errors. In this work, we introduce the APHYNITY framework, a principled approach for augmenting incomplete physical dynamics described by differential equations with deep data-driven models. It consists in decomposing the dynamics into two components: a physical component accounting for the dynamics for which we have some prior knowledge, and a data-driven component accounting for errors of the physical model. The learning problem is carefully formulated such that the physical model explains as much of the data as possible, while the data-driven component only describes information that cannot be captured by the physical model, no more, no less. This not only provides the existence and uniqueness for this decomposition, but also ensures interpretability and benefits generalization. Experiments made on three important use cases, each representative of a different family of phenomena, i.e. reaction-diffusion equations, wave equations and the non-linear damped pendulum, show that APHYNITY can efficiently leverage approximate physical models to accurately forecast the evolution of the system and correctly identify relevant physical parameters.},
  groups   = {R32 Bosch},
  langid   = {english},
}

@Article{Zaheer18DeepSets,
  author        = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander},
  title         = {Deep {{Sets}}},
  year          = {2018},
  month         = apr,
  abstract      = {We study the problem of designing models for machine learning tasks defined on sets. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics [1], to anomaly detection in piezometer data of embankment dams [2], to cosmology [3, 4]. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
  archiveprefix = {arXiv},
  eprint        = {1703.06114},
  eprintclass   = {cs,stat},
  eprinttype    = {arxiv},
  file          = {:Zaheer18 - Deep Sets.pdf:PDF},
  groups        = {R32 Bosch},
  keywords      = {Computer Science - Machine Learning, Statistics - Machine Learning},
  langid        = {english},
  primaryclass  = {cs, stat},
}

@InProceedings{Bordelon20Spectrumdependentlearning,
  author       = {Bordelon, Blake and Canatar, Abdulkadir and Pehlevan, Cengiz},
  booktitle    = {International Conference on Machine Learning},
  title        = {Spectrum dependent learning curves in kernel regression and wide neural networks},
  year         = {2020},
  organization = {PMLR},
  pages        = {1024--1034},
  file         = {:bordelon20a-2.pdf:PDF},
  groups       = {bayesian chaos},
}

@Article{Sompolinsky88Chaosrandomneural,
  author           = {Sompolinsky, Haim and Crisanti, Andrea and Sommers, Hans-Jurgen},
  journal          = {Physical review letters},
  title            = {Chaos in random neural networks},
  year             = {1988},
  number           = {3},
  pages            = {259},
  volume           = {61},
  creationdate     = {2022-03-13T11:57:03},
  file             = {:Sompolinsky88 - Chaos in Random Neural Networks.pdf:PDF},
  groups           = {bayesian chaos},
  modificationdate = {2022-03-13T11:57:45},
  publisher        = {APS},
}

@Book{Murphy22Probabilisticmachinelearning,
  author           = {Murphy, Kevin P},
  publisher        = {MIT press},
  title            = {Probabilistic machine learning: an introduction},
  year             = {2022},
  creationdate     = {2022-03-14T10:18:42},
  file             = {:Murphy22 - Probabilistic Machine Learning_ an Introduction.pdf:PDF},
  groups           = {bayesian chaos},
  modificationdate = {2022-03-14T10:18:51},
}

@Article{Geroch97Suggestionsgivingtalks,
  author           = {Geroch, Robert},
  journal          = {arXiv preprint gr-qc/9703019},
  title            = {Suggestions for giving talks},
  year             = {1973},
  creationdate     = {2022-03-16T13:46:38},
  file             = {:Geroch97 - Suggestions for Giving Talks.pdf:PDF},
  groups           = {misc},
  modificationdate = {2022-04-02T15:03:01},
}

@Article{Braun05Spectralpropertieskernel,
  author           = {Braun, Mikio Ludwig},
  title            = {Spectral properties of the kernel matrix and their relation to kernel methods in machine learning},
  year             = {2005},
  creationdate     = {2022-03-21T16:05:02},
  groups           = {bayesian chaos},
  modificationdate = {2022-03-21T16:05:02},
  publisher        = {Universit{\"a}ts-und Landesbibliothek Bonn},
}

@Article{Koltchinskii00Randommatrixapproximation,
  author           = {Koltchinskii, Vladimir and Gin{\'e}, Evarist},
  journal          = {Bernoulli},
  title            = {Random matrix approximation of spectra of integral operators},
  year             = {2000},
  pages            = {113--167},
  creationdate     = {2022-03-21T16:05:06},
  groups           = {bayesian chaos},
  modificationdate = {2022-03-21T16:05:06},
  publisher        = {JSTOR},
}

@Article{Schuster10Applicationgametheory,
  author           = {Schuster, Alfons and Yamaguchi, Yoko},
  journal          = {Advances in Artificial Intelligence},
  title            = {Application of game theory to neuronal networks},
  year             = {2010},
  volume           = {2010},
  creationdate     = {2022-03-22T14:47:34},
  file             = {:Schuster10 - Application of Game Theory to Neuronal Networks.pdf:PDF},
  modificationdate = {2022-03-22T14:47:47},
  publisher        = {Hindawi},
}

@Article{Canatar21OutDistributionGeneralization,
  author           = {Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
  journal          = {Advances in Neural Information Processing Systems},
  title            = {Out-of-Distribution Generalization in Kernel Regression},
  year             = {2021},
  volume           = {34},
  creationdate     = {2022-03-29T17:04:50},
  file             = {:Canatar21 - Out of Distribution Generalization in Kernel Regression.pdf:PDF},
  groups           = {bayesian chaos},
  modificationdate = {2022-03-29T17:05:11},
}

@Article{Schuecker18Optimalsequencememory,
  author           = {Schuecker, Jannis and Goedeke, Sven and Helias, Moritz},
  journal          = {Physical Review X},
  title            = {Optimal sequence memory in driven random networks},
  year             = {2018},
  number           = {4},
  pages            = {041029},
  volume           = {8},
  creationdate     = {2022-03-30T11:40:36},
  file             = {:Schuecker18 - Optimal Sequence Memory in Driven Random Networks.pdf:PDF},
  groups           = {bayesian chaos},
  modificationdate = {2022-03-30T11:41:04},
  publisher        = {APS},
}

@Article{Krishnamurthy22Theorygatingrecurrent,
  author           = {Krishnamurthy, Kamesh and Can, Tankut and Schwab, David J},
  journal          = {Physical Review X},
  title            = {Theory of gating in recurrent neural networks},
  year             = {2022},
  number           = {1},
  pages            = {011011},
  volume           = {12},
  creationdate     = {2022-03-31T19:20:21},
  file             = {:Krishnamurthy22 - Theory of Gating in Recurrent Neural Networks.pdf:PDF},
  groups           = {PDLT},
  modificationdate = {2022-03-31T19:20:26},
  publisher        = {APS},
}

@Article{VandenOord18Representationlearningcontrastive,
  author           = {Van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol and others},
  journal          = {arXiv preprint arXiv:1807.03748},
  title            = {Representation learning with contrastive predictive coding},
  year             = {2018},
  number           = {3},
  pages            = {4},
  volume           = {2},
  creationdate     = {2022-04-02T18:57:45},
  file             = {:Van den Oord18 - Representation Learning with Contrastive Predictive Coding.pdf:PDF},
  groups           = {RL},
  modificationdate = {2022-04-02T18:58:17},
}

@Article{Friston18Doespredictivecoding,
  author           = {Friston, Karl},
  journal          = {Nature neuroscience},
  title            = {Does predictive coding have a future?},
  year             = {2018},
  number           = {8},
  pages            = {1019--1021},
  volume           = {21},
  creationdate     = {2022-04-02T18:58:13},
  file             = {:Friston18 - Does Predictive Coding Have a Future_.pdf:PDF},
  groups           = {RL},
  modificationdate = {2022-04-02T18:58:23},
  publisher        = {Nature Publishing Group},
}

@Article{Chizat18notelazytraininga,
  author           = {Chizat, Lenaic and Bach, Francis},
  journal          = {arXiv preprint arXiv:1812.07956},
  title            = {A note on lazy training in supervised differentiable programming},
  year             = {2018},
  volume           = {8},
  creationdate     = {2022-04-05T09:37:23},
  file             = {:Chizat18 - A Note on Lazy Training in Supervised Differentiable Programming.pdf:PDF},
  groups           = {JAJ},
  modificationdate = {2022-04-05T09:38:23},
}

@Article{Chizat19lazytrainingdifferentiable,
  author           = {Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal          = {Advances in Neural Information Processing Systems},
  title            = {On lazy training in differentiable programming},
  year             = {2019},
  volume           = {32},
  comment          = {Introduced the lazy regime},
  creationdate     = {2022-04-05T09:37:31},
  file             = {:Chizat19 - On Lazy Training in Differentiable Programming.pdf:PDF},
  groups           = {JAJ},
  modificationdate = {2022-04-25T11:48:02},
}

@Article{Gallego17Neuralmanifoldscontrol,
  author           = {Gallego, Juan A and Perich, Matthew G and Miller, Lee E and Solla, Sara A},
  journal          = {Neuron},
  title            = {Neural manifolds for the control of movement},
  year             = {2017},
  number           = {5},
  pages            = {978--984},
  volume           = {94},
  creationdate     = {2022-04-08T15:28:08},
  file             = {:Gallego17 - Neural Manifolds for the Control of Movement.pdf:PDF},
  modificationdate = {2022-04-08T15:28:39},
  publisher        = {Elsevier},
}

@Article{Naveh21selfconsistenttheory,
  author           = {Naveh, Gadi and Ringel, Zohar},
  journal          = {Advances in Neural Information Processing Systems},
  title            = {A self consistent theory of Gaussian Processes captures feature learning effects in finite CNNs},
  year             = {2021},
  volume           = {34},
  creationdate     = {2022-04-10T18:23:18},
  file             = {:Naveh21 - A Self Consistent Theory of Gaussian Processes Captures Feature Learning Effects in Finite CNNs.pdf:PDF},
  groups           = {PDLT},
  modificationdate = {2022-04-12T11:28:45},
  readstatus       = {read},
}

@Article{Beggs12Beingcriticalcriticality,
  author           = {Beggs, John M and Timme, Nicholas},
  journal          = {Frontiers in physiology},
  title            = {Being critical of criticality in the brain},
  year             = {2012},
  pages            = {163},
  volume           = {3},
  creationdate     = {2022-04-11T15:27:10},
  file             = {:Beggs12 - Being Critical of Criticality in the Brain.pdf:PDF},
  modificationdate = {2022-04-11T15:27:21},
  publisher        = {Frontiers Research Foundation},
}

@InProceedings{Woodworth20Kernelrichregimes,
  author           = {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle        = {Conference on Learning Theory},
  title            = {Kernel and rich regimes in overparametrized models},
  year             = {2020},
  organization     = {PMLR},
  pages            = {3635--3673},
  creationdate     = {2022-04-11T23:13:32},
  file             = {:Woodworth20 - Kernel and Rich Regimes in Overparametrized Models.pdf:PDF},
  groups           = {RNFlows JC},
  modificationdate = {2022-04-16T10:41:13},
  priority         = {prio2},
}

@Article{Dahmen16Correlatedfluctuationsstrongly,
  author           = {Dahmen, David and Bos, Hannah and Helias, Moritz},
  journal          = {Physical Review X},
  title            = {Correlated fluctuations in strongly coupled binary networks beyond equilibrium},
  year             = {2016},
  number           = {3},
  pages            = {031024},
  volume           = {6},
  creationdate     = {2022-04-24T17:21:26},
  file             = {:Dahmen16 - Correlated Fluctuations in Strongly Coupled Binary Networks beyond Equilibrium.pdf:PDF},
  groups           = {bayesian chaos},
  modificationdate = {2022-04-24T17:21:38},
  publisher        = {APS},
}

@Article{Mainen95Reliabilityspiketiming,
  author           = {Mainen, Zachary F and Sejnowski, Terrence J},
  journal          = {Science},
  title            = {Reliability of spike timing in neocortical neurons},
  year             = {1995},
  number           = {5216},
  pages            = {1503--1506},
  volume           = {268},
  creationdate     = {2022-04-24T17:25:20},
  file             = {:Mainen95 - Reliability of Spike Timing in Neocortical Neurons.pdf:PDF},
  groups           = {bayesian chaos},
  modificationdate = {2022-04-24T17:25:52},
  publisher        = {American Association for the Advancement of Science},
}

@InProceedings{DAscoli20Doubletroubledoubledescent,
  author           = {d’Ascoli, St{\'e}phane and Refinetti, Maria and Biroli, Giulio and Krzakala, Florent},
  booktitle        = {International Conference on Machine Learning},
  title            = {Double trouble in double descent: Bias and variance (s) in the lazy regime},
  year             = {2020},
  organization     = {PMLR},
  pages            = {2280--2290},
  creationdate     = {2022-04-25T11:41:19},
  file             = {:20 - Double Trouble in Double Descent_ Bias and Variance (s) in the Lazy Regime.pdf:PDF},
  modificationdate = {2022-04-25T11:42:05},
}

@Article{Seroussi21Separationscalesthermodynamic,
  author           = {Seroussi, Inbar and Ringel, Zohar},
  journal          = {arXiv preprint arXiv:2112.15383},
  title            = {Separation of scales and a thermodynamic description of feature learning in some CNNs},
  year             = {2021},
  creationdate     = {2022-04-26T10:33:11},
  file             = {:Seroussi21 - Separation of Scales and a Thermodynamic Description of Feature Learning in Some CNNs.pdf:PDF},
  groups           = {JAJ},
  modificationdate = {2022-04-26T10:33:25},
}

@Article{Hu19Simpleeffectiveregularization,
  author           = {Hu, Wei and Li, Zhiyuan and Yu, Dingli},
  journal          = {arXiv preprint arXiv:1905.11368},
  title            = {Simple and effective regularization methods for training on noisily labeled data with generalization guarantee},
  year             = {2019},
  comment          = {show that NTK is realized by RR},
  creationdate     = {2022-04-27T17:49:26},
  file             = {:Hu19 - Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee.pdf:PDF},
  groups           = {bayesian chaos},
  modificationdate = {2022-04-27T17:49:50},
}

@Article{Livan18Introductionrandommatrices,
  author           = {Livan, Giacomo and Novaes, Marcel and Vivo, Pierpaolo},
  journal          = {Monograph Award},
  title            = {Introduction to random matrices theory and practice},
  year             = {2018},
  pages            = {63},
  creationdate     = {2022-04-27T22:32:26},
  file             = {:Livan18 - Introduction to Random Matrices Theory and Practice.pdf:PDF},
  modificationdate = {2022-04-27T22:32:37},
}

@Book{ProbabilisticMachineLearning,
  author           = {Kevin P. Murphy},
  publisher        = {MIT Press},
  title            = {Probabilistic Machine Learning: Advanced Topics},
  year             = {2023},
  creationdate     = {2022-04-30T17:19:14},
  file             = {:Murphy23 - Probabilistic Machine Learning_ Advanced Topics.pdf:PDF},
  modificationdate = {2022-06-02T19:15:37},
  url              = {probml.ai},
}

@Article{Rahimi07Randomfeatureslarge,
  author           = {Rahimi, Ali and Recht, Benjamin},
  journal          = {Advances in neural information processing systems},
  title            = {Random features for large-scale kernel machines},
  year             = {2007},
  volume           = {20},
  creationdate     = {2022-05-02T09:33:49},
  file             = {:Rahimi07 - Random Features for Large Scale Kernel Machines.pdf:PDF},
  modificationdate = {2022-05-02T09:34:06},
}

@Book{Gardiner85Handbookstochasticmethods,
  author           = {Gardiner, Crispin W and others},
  publisher        = {springer Berlin},
  title            = {Handbook of stochastic methods},
  year             = {1985},
  volume           = {3},
  creationdate     = {2022-05-06T10:43:07},
  file             = {:Gardiner85 - Handbook of Stochastic Methods.pdf:PDF},
  modificationdate = {2022-05-06T10:43:17},
}

@Book{Cox62Renewaltheory,
  author           = {Cox, David Roxbee},
  publisher        = {Methuen},
  title            = {Renewal theory},
  year             = {1962},
  creationdate     = {2022-05-06T10:58:00},
  groups           = {bayesian chaos},
  modificationdate = {2022-05-06T11:03:21},
}

@MastersThesis{Keup17NeuronModelIndependent,
  author           = {Christian Keup},
  title            = {A Neuron Model Independent Path Integral Explored via Binary Assemblies},
  year             = {2017},
  creationdate     = {2022-05-08T16:45:03},
  file             = {:Keup17 - A Neuron Model Independent Path Integral Explored Via Binary Assemblies.pdf:PDF},
  modificationdate = {2022-05-10T14:57:00},
}

@MastersThesis{Fischer20DecompositionDeepNeural,
  author           = {Kirsten Fischer},
  title            = {Decomposition of Deep Neural Networks into Correlation Functions},
  year             = {2020},
  creationdate     = {2022-05-08T16:46:14},
  file             = {:Fischer20 - Decomposition of Deep Neural Networks into Correlation Functions.pdf:PDF},
  modificationdate = {2022-05-08T16:47:50},
}

@Article{Morales21Optimalinputrepresentation,
  author           = {Morales, Guillermo B and Mu{\~n}oz, Miguel A},
  journal          = {Biology},
  title            = {Optimal input representation in neural systems at the edge of chaos},
  year             = {2021},
  number           = {8},
  pages            = {702},
  volume           = {10},
  creationdate     = {2022-05-09T16:22:23},
  file             = {:Morales21 - Optimal Input Representation in Neural Systems at the Edge of Chaos.pdf:PDF},
  modificationdate = {2022-05-09T16:22:40},
  publisher        = {Multidisciplinary Digital Publishing Institute},
}

@Article{Cohen22Softmarginclassification,
  author           = {Cohen, Uri and Sompolinsky, Haim},
  journal          = {arXiv preprint arXiv:2203.07040},
  title            = {Soft-margin classification of object manifolds},
  year             = {2022},
  creationdate     = {2022-05-10T16:56:21},
  file             = {:Cohen22 - Soft Margin Classification of Object Manifolds.pdf:PDF},
  groups           = {JAJ},
  modificationdate = {2022-05-10T16:56:36},
}

@Article{Nolte19Corticalreliabilityamid,
  author           = {Nolte, Max and Reimann, Michael W and King, James G and Markram, Henry and Muller, Eilif B},
  journal          = {Nature communications},
  title            = {Cortical reliability amid noise and chaos},
  year             = {2019},
  number           = {1},
  pages            = {1--15},
  volume           = {10},
  creationdate     = {2022-05-13T11:13:20},
  file             = {:Nolte19 - Cortical Reliability Amid Noise and Chaos.pdf:PDF},
  modificationdate = {2022-05-13T11:13:30},
  publisher        = {Nature Publishing Group},
}

@Book{Couillet22RandomMatrixMethods,
  author           = {Couillet, Romain and Liao, Zhenyu},
  publisher        = {Cambridge University Press},
  title            = {Random Matrix Methods for Machine Learning},
  year             = {2022},
  creationdate     = {2022-05-13T11:55:05},
  file             = {:/Users/jan/Library/CloudStorage/OneDrive-rwth-aachen.de/literature/RMT4ML.pdf:PDF},
  modificationdate = {2022-05-13T20:45:48},
  place            = {Cambridge},
}

@Misc{Bordelon22SelfConsistentDynamical,
  author           = {Bordelon, Blake and Pehlevan, Cengiz},
  title            = {Self-Consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks},
  year             = {2022},
  copyright        = {Creative Commons Attribution 4.0 International},
  creationdate     = {2022-05-21T08:11:37},
  doi              = {10.48550/ARXIV.2205.09653},
  file             = {:Bordelon22 - Self Consistent Dynamical Field Theory of Kernel Evolution in Wide Neural Networks.pdf:PDF},
  keywords         = {Machine Learning (stat.ML), Disordered Systems and Neural Networks (cond-mat.dis-nn), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Physical sciences},
  modificationdate = {2022-05-21T08:12:06},
  publisher        = {arXiv},
  url              = {https://arxiv.org/abs/2205.09653},
}

@Article{Touchette05LegendreFencheltransforms,
  author           = {Hugo Touchette},
  title            = {Legendre-Fenchel transforms in a nutshell},
  year             = {2005},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
}

@Article{MontgomerySmith13convexitylogmoment,
  author           = {Stephen Montgomery-Smith},
  title            = {convexity of log of moment generating function},
  year             = {2013},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  url              = {https://math.stackexchange.com/questions/600908/convexity-of-log-of-moment-generating-function},
}

@Article{Gabrie15TrainingRestrictedBoltzmann,
  author           = {Gabrie, Marylou and Tramel, Eric and Krzakala, Florent},
  title            = {Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer Free Energy},
  year             = {2015},
  month            = {06},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
}

@Book{JoelLebowitz00PhaseTransitionsCritical,
  author           = {Joel Lebowitz, Cyril Domb},
  title            = {Phase Transitions and Critical Phenomena},
  year             = {2000},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  url              = {https://www.mv.helsinki.fi/home/rummukai/simu/expansions.pdfhttps://www.mv.helsinki.fi/home/rummukai/simu/expansions.pdf},
}

@Book{Balian07MicrophysicsMacrophysics,
  author           = {Roger Balian},
  title            = {From Microphysics to Macrophysics},
  year             = {2007},
  volume           = {1},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
}

@Article{Sherrington75SolvableModelSpin,
  author           = {Sherrington, David and Kirkpatrick, Scott},
  journal          = {Phys. Rev. Lett.},
  title            = {Solvable Model of a Spin-Glass},
  year             = {1975},
  month            = {Dec},
  pages            = {1792--1796},
  volume           = {35},
  creationdate     = {2022-05-27T21:21:06},
  doi              = {10.1103/PhysRevLett.35.1792},
  groups           = {rbm_diagrams},
  issue            = {26},
  modificationdate = {2022-05-27T21:21:06},
  numpages         = {0},
  publisher        = {American Physical Society},
  url              = {https://link.aps.org/doi/10.1103/PhysRevLett.35.1792},
}

@Book{ZinnJustin02QuantumFieldTheory,
  author           = {Zinn-Justin, J.},
  publisher        = {Clarendon Press},
  title            = {Quantum Field Theory and Critical Phenomena},
  year             = {2002},
  isbn             = {9780198509233},
  series           = {International series of monographs on physics},
  creationdate     = {2022-05-27T21:21:06},
  file             = {:/Users/jan/Downloads/zinn-justin - Quantum Field Theory and Critical Phenomena.txt:Text},
  groups           = {rbm_diagrams},
  lccn             = {96013457},
  modificationdate = {2022-05-27T21:21:06},
  url              = {https://books.google.de/books?id=N8DBpTzBCJYC},
}

@Article{Parisi79InfiniteNumberOrder,
  author           = {Parisi, G.},
  journal          = {Phys. Rev. Lett.},
  title            = {Infinite Number of Order Parameters for Spin-Glasses},
  year             = {1979},
  month            = {Dec},
  pages            = {1754--1756},
  volume           = {43},
  creationdate     = {2022-05-27T21:21:06},
  doi              = {10.1103/PhysRevLett.43.1754},
  groups           = {rbm_diagrams},
  issue            = {23},
  modificationdate = {2022-05-27T21:21:06},
  numpages         = {0},
  publisher        = {American Physical Society},
  url              = {https://link.aps.org/doi/10.1103/PhysRevLett.43.1754},
}

@Article{Zia09MakingsenseLegendre,
  author           = {Zia, Royce KP and Redish, Edward F and McKay, Susan R},
  journal          = {American Journal of Physics},
  title            = {Making sense of the Legendre transform},
  year             = {2009},
  number           = {7},
  pages            = {614--622},
  volume           = {77},
  creationdate     = {2022-05-27T21:21:06},
  file             = {:/Users/jan/Downloads/makingSenseLF - Making Sense of the Legendre Transform.bib:bib;:/Users/jan/Downloads/makingSenseLF - Making Sense of the Legendre Transform.txt:Text},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  publisher        = {AAPT},
}

@Book{Hertz91IntroductionTheoryNeural,
  author           = {Hertz, J and , John and , Krough and Flisberg, Anders and , Palmer and G, Richard},
  title            = {Introduction To The Theory Of Neural Computation},
  year             = {1991},
  month            = {01},
  volume           = {44},
  creationdate     = {2022-05-27T21:21:06},
  doi              = {10.1063/1.2810360},
  groups           = {rbm_diagrams},
  journal          = {Physics Today - PHYS TODAY},
  modificationdate = {2022-05-27T21:21:06},
}

@Article{Hopfield82NeuralNetworksPhysical,
  author           = {Hopfield, John},
  journal          = {Proceedings of the National Academy of Sciences of the United States of America},
  title            = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities},
  year             = {1982},
  month            = {05},
  pages            = {2554-8},
  volume           = {79},
  creationdate     = {2022-05-27T21:21:06},
  doi              = {10.1073/pnas.79.8.2554},
  file             = {:/Users/jan/Downloads/hopfield - Neural Networks and Physical Systems with Emergent Collective Computational Abilities.bib:bib;:/Users/jan/Downloads/scholar-8.bib:bib;:/Users/jan/Downloads/scholar-17.bib:bib;:/Users/jan/Downloads/scholar-18.bib:bib},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
}

@Article{Hinton06ReducingDimensionalityData,
  author           = {Hinton, G. E. and Salakhutdinov, R. R.},
  journal          = {Science},
  title            = {Reducing the Dimensionality of Data with Neural Networks},
  year             = {2006},
  issn             = {0036-8075},
  number           = {5786},
  pages            = {504--507},
  volume           = {313},
  abstract         = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such {\textquotedblleft}autoencoder{\textquotedblright} networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  creationdate     = {2022-05-27T21:21:06},
  doi              = {10.1126/science.1127647},
  eprint           = {https://science.sciencemag.org/content/313/5786/504.full.pdf},
  file             = {:/Users/jan/Downloads/netflix - Reducing the Dimensionality of Data with Neural Networks.bib:bib;:/Users/jan/Downloads/netflix - Reducing the Dimensionality of Data with Neural Networks.txt:Text;:/Users/jan/Downloads/scholar-16.bib:bib},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  publisher        = {American Association for the Advancement of Science},
  url              = {https://science.sciencemag.org/content/313/5786/504},
}

@InProceedings{Salakhutdinov07RestrictedBoltzmannMachines,
  author           = {Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey},
  booktitle        = {Proceedings of the 24th International Conference on Machine Learning},
  title            = {Restricted Boltzmann Machines for Collaborative Filtering},
  year             = {2007},
  address          = {New York, NY, USA},
  pages            = {791--798},
  publisher        = {ACM},
  series           = {ICML '07},
  acmid            = {1273596},
  creationdate     = {2022-05-27T21:21:06},
  doi              = {10.1145/1273496.1273596},
  file             = {:/Users/jan/Downloads/Salakhutdinov2007 - Restricted Boltzmann Machines for Collaborative Filtering.bib:bib;:/Users/jan/Downloads/scholar-5.bib:bib},
  groups           = {rbm_diagrams},
  isbn             = {978-1-59593-793-3},
  location         = {Corvalis, Oregon, USA},
  modificationdate = {2022-05-27T21:21:06},
  numpages         = {8},
  url              = {http://doi.acm.org/10.1145/1273496.1273596},
}

@InProceedings{Fischer12IntroductionRestrictedBoltzmann,
  author           = {Fischer, Asja and Igel, Christian},
  title            = {An Introduction to Restricted Boltzmann Machines},
  year             = {2012},
  month            = {01},
  pages            = {14-36},
  creationdate     = {2022-05-27T21:21:06},
  doi              = {10.1007/978-3-642-33275-3_2},
  file             = {:/Users/jan/Downloads/Fischer2012 - An Introduction to Restricted Boltzmann Machines.bib:bib;:/Users/jan/Downloads/Fischer2012 - An Introduction to Restricted Boltzmann Machines.bibtex:bibtex;:/Users/jan/Downloads/scholar-9.bib:bib},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
}

@InProceedings{Tieleman08TrainingRestrictedBoltzmann,
  author           = {Tieleman, Tijmen},
  booktitle        = {Proceedings of the 25th International Conference on Machine Learning},
  title            = {Training Restricted Boltzmann Machines Using Approximations to the Likelihood Gradient},
  year             = {2008},
  address          = {New York, NY, USA},
  pages            = {1064--1071},
  publisher        = {ACM},
  series           = {ICML '08},
  acmid            = {1390290},
  creationdate     = {2022-05-27T21:21:06},
  doi              = {10.1145/1390156.1390290},
  file             = {:/Users/jan/Downloads/Tieleman2008 - Training Restricted Boltzmann Machines Using Approximations to the Likelihood Gradient.bibtex:bibtex},
  groups           = {rbm_diagrams},
  isbn             = {978-1-60558-205-4},
  location         = {Helsinki, Finland},
  modificationdate = {2022-05-27T21:21:06},
  numpages         = {8},
  url              = {http://doi.acm.org/10.1145/1390156.1390290},
}

@Article{Hinton02Trainingproductsexperts,
  author           = {Hinton, Geoffrey},
  journal          = {Neural computation},
  title            = {Training products of experts by minimizing contrastive divergence},
  year             = {2002},
  number           = {8},
  pages            = {1771--1800},
  volume           = {14},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  publisher        = {MIT Press},
}

@Article{Mehta15Statisticsstationarypoints,
  author           = {Mehta, Dhagash and Niemerg, Matthew and Sun, Chuang},
  journal          = {Journal of Statistical Mechanics: Theory and Experiment},
  title            = {Statistics of stationary points of random finite polynomial potentials},
  year             = {2015},
  number           = {9},
  pages            = {P09012},
  volume           = {2015},
  creationdate     = {2022-05-27T21:21:06},
  file             = {:/Users/jan/Downloads/mvpoly - Statistics of Stationary Points of Random Finite Polynomial Potentials.bib:bib},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  publisher        = {IOP Publishing},
}

@Article{Glauber63Timedependentstatistics,
  author           = {Glauber, Roy J},
  journal          = {Journal of mathematical physics},
  title            = {Time-dependent statistics of the Ising model},
  year             = {1963},
  number           = {2},
  pages            = {294--307},
  volume           = {4},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  publisher        = {AIP},
}

@Book{Opper01AdvancedMeanField,
  author           = {Opper, M. and Saad, D.},
  publisher        = {MIT Press},
  title            = {Advanced Mean Field Methods: Theory and Practice},
  year             = {2001},
  isbn             = {9780262150545},
  series           = {Neural information processing series},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  lccn             = {00053322},
  modificationdate = {2022-05-27T21:21:06},
  url              = {https://books.google.de/books?id=cuOX8sCDeNAC},
}

@Article{LeRoux08Representationalpowerrestricted,
  author           = {Le Roux, Nicolas and Bengio, Yoshua},
  journal          = {Neural computation},
  title            = {Representational power of restricted Boltzmann machines and deep belief networks},
  year             = {2008},
  number           = {6},
  pages            = {1631--1649},
  volume           = {20},
  creationdate     = {2022-05-27T21:21:06},
  file             = {:/Users/jan/Downloads/LeRoux2008 - Representational Power of Restricted Boltzmann Machines and Deep Belief Networks.bib:bib},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  publisher        = {MIT Press},
}

@Article{Kuehn18Expansioneffectiveaction,
  author           = {K{\"u}hn, Tobias and Helias, Moritz},
  journal          = {Journal of Physics A: Mathematical and Theoretical},
  title            = {Expansion of the effective action around non-Gaussian theories},
  year             = {2018},
  number           = {37},
  pages            = {375004},
  volume           = {51},
  creationdate     = {2022-05-27T21:21:06},
  file             = {:Kühn18 - Expansion of the Effective Action around Non Gaussian Theories.pdf:PDF},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:23:08},
  publisher        = {IOP Publishing},
}

@Article{Helias19Statisticalfieldtheory,
  author           = {Helias, M AND Dahmen, D},
  title            = {Statistical field theory for neural networks},
  year             = {2019},
  month            = jan,
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  url              = {https://arxiv.org/abs/1901.10416},
}

@Article{Mehta14exactmappingvariational,
  author           = {Mehta, Pankaj and Schwab, David J},
  journal          = {arXiv preprint arXiv:1410.3831},
  title            = {An exact mapping between the variational renormalization group and deep learning},
  year             = {2014},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
}

@Article{Mehta19highbiaslow,
  author           = {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre GR and Richardson, Clint and Fisher, Charles K and Schwab, David J},
  journal          = {Physics Reports},
  title            = {A high-bias, low-variance introduction to machine learning for physicists},
  year             = {2019},
  creationdate     = {2022-05-27T21:21:06},
  file             = {:/Users/jan/Downloads/HighBiasLowVariance - A High Bias, Low Variance Introduction to Machine Learning for Physicists.bib:bib},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  publisher        = {Elsevier},
}

@TechReport{Smolensky86Informationprocessingdynamical,
  author           = {Smolensky, Paul},
  title            = {Information processing in dynamical systems: Foundations of harmony theory},
  year             = {1986},
  creationdate     = {2022-05-27T21:21:06},
  file             = {:/Users/jan/Downloads/Smolensky1986 - Information Processing in Dynamical Systems_ Foundations of Harmony Theory.txt:Text},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  school           = {Colorado Univ at Boulder Dept of Computer Science},
}

@Article{Domino18EfficientComputationHigher,
  author           = {Domino, Krzysztof and Gawron, Piotr and Pawela, {\L}ukasz},
  journal          = {SIAM Journal on Scientific Computing},
  title            = {Efficient Computation of Higher-Order Cumulant Tensors},
  year             = {2018},
  number           = {3},
  pages            = {A1590--A1610},
  volume           = {40},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  publisher        = {SIAM},
}

@Article{Tramel18Deterministicgeneralizedframework,
  author           = {Tramel, Eric W and Gabri{\'e}, Marylou and Manoel, Andre and Caltagirone, Francesco and Krzakala, Florent},
  journal          = {Physical Review X},
  title            = {Deterministic and generalized framework for unsupervised learning with restricted boltzmann machines},
  year             = {2018},
  number           = {4},
  pages            = {041006},
  volume           = {8},
  creationdate     = {2022-05-27T21:21:06},
  file             = {:/Users/jan/Downloads/Gabrie2018 - Deterministic and Generalized Framework for Unsupervised Learning with Restricted Boltzmann Machines.bib:bib},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  publisher        = {APS},
}

@Book{Nishimori01Statisticalphysicsspin,
  author           = {Nishimori, Hidetoshi},
  publisher        = {Clarendon Press},
  title            = {Statistical physics of spin glasses and information processing: an introduction},
  year             = {2001},
  number           = {111},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
}

@Article{Castellani05Spinglasstheory,
  author           = {Castellani, Tommaso and Cavagna, Andrea},
  journal          = {Journal of Statistical Mechanics: Theory and Experiment},
  title            = {Spin-glass theory for pedestrians},
  year             = {2005},
  number           = {05},
  pages            = {P05012},
  volume           = {2005},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  publisher        = {IOP Publishing},
}

@Article{Shannon48mathematicaltheorycommunication,
  author           = {Shannon, Claude Elwood},
  journal          = {Bell system technical journal},
  title            = {A mathematical theory of communication},
  year             = {1948},
  number           = {3},
  pages            = {379--423},
  volume           = {27},
  creationdate     = {2022-05-27T21:21:06},
  file             = {:/Users/jan/Downloads/Shannon1948 - A Mathematical Theory of Communication.bib:bib;:/Users/jan/Downloads/scholar-12.bib:bib},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  publisher        = {Wiley Online Library},
}

@Article{Jaynes57Informationtheorystatistical,
  author           = {Jaynes, Edwin T},
  journal          = {Physical review},
  title            = {Information theory and statistical mechanics},
  year             = {1957},
  number           = {4},
  pages            = {620},
  volume           = {106},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  publisher        = {APS},
}

@Article{Jaynes57Informationtheorystatisticala,
  author           = {Jaynes, Edwin T},
  journal          = {Physical review},
  title            = {Information theory and statistical mechanics. II},
  year             = {1957},
  number           = {2},
  pages            = {171},
  volume           = {108},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:06},
  publisher        = {APS},
}

@Book{Gibbs14Elementaryprinciplesstatistical,
  author           = {Gibbs, J Willard},
  publisher        = {Courier Corporation},
  title            = {Elementary principles in statistical mechanics},
  year             = {2014},
  creationdate     = {2022-05-27T21:21:06},
  file             = {:/Users/jan/Downloads/Gibbs - Elementary Principles in Statistical Mechanics.bib:bib;:/Users/jan/Downloads/scholar-14.bib:bib;:/Users/jan/Downloads/scholar-15.bib:bib},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:07},
}

@Book{Kandel00Principlesneuralscience,
  author           = {Kandel, Eric R and Schwartz, James H and Jessell, Thomas M and Department of Biochemistry and Molecular Biophysics Thomas Jessell and Siegelbaum, Steven and Hudspeth, AJ},
  publisher        = {McGraw-hill New York},
  title            = {Principles of neural science},
  year             = {2000},
  volume           = {4},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:07},
}

@Article{Hinton12Nguyen367P.,
  author           = {Hinton, Geoffrey and Deng, L and Yu, D and Dahl, GE and Mohamed, A-r and Jaitly, N and Senior, A and Vanhoucke, V},
  journal          = {IEEE Signal Processing Magazine},
  title            = {Nguyen, 367 P., Sainath, TN, and Kingsbury, B.“Deep neural networks for acoustic modeling in speech 368 recognition,”},
  year             = {2012},
  pages            = {82--97},
  volume           = {29},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:07},
}

@InProceedings{Krizhevsky12Imagenetclassificationdeep,
  author           = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey},
  booktitle        = {Advances in neural information processing systems},
  title            = {Imagenet classification with deep convolutional neural networks},
  year             = {2012},
  pages            = {1097--1105},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:07},
}

@Article{Georges91Howexpandmean,
  author           = {Georges, Antoine and Yedidia, Jonathan S},
  journal          = {Journal of Physics A: Mathematical and General},
  title            = {How to expand around mean-field theory using high-temperature expansions},
  year             = {1991},
  number           = {9},
  pages            = {2173},
  volume           = {24},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:07},
  publisher        = {IOP Publishing},
}

@Article{Plefka82ConvergenceconditionTAP,
  author           = {Plefka, Timm},
  journal          = {Journal of Physics A: Mathematical and general},
  title            = {Convergence condition of the TAP equation for the infinite-ranged Ising spin glass model},
  year             = {1982},
  number           = {6},
  pages            = {1971},
  volume           = {15},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:07},
  publisher        = {IOP Publishing},
}

@Article{Ising25Beitragzurtheorie,
  author           = {Ising, Ernst},
  journal          = {Zeitschrift f{\"u}r Physik A Hadrons and Nuclei},
  title            = {Beitrag zur theorie des ferromagnetismus},
  year             = {1925},
  number           = {1},
  pages            = {253--258},
  volume           = {31},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:07},
  publisher        = {Springer},
}

@Article{Derrida80Randomenergymodel,
  author           = {Derrida, Bernard},
  journal          = {Physical Review Letters},
  title            = {Random-energy model: Limit of a family of disordered models},
  year             = {1980},
  number           = {2},
  pages            = {79},
  volume           = {45},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:07},
  publisher        = {APS},
}

@Article{Euler41Solutioproblematisad,
  author           = {Euler, Leonhard},
  journal          = {Commentarii academiae scientiarum Petropolitanae},
  title            = {Solutio problematis ad geometriam situs pertinentis},
  year             = {1741},
  pages            = {128--140},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:07},
}

@Article{McCullagh09Cumulants,
  author           = {McCullagh, P. and Kolassa, J.},
  journal          = {Scholarpedia},
  title            = {{C}umulants},
  year             = {2009},
  note             = {revision \#137322},
  number           = {3},
  pages            = {4699},
  volume           = {4},
  creationdate     = {2022-05-27T21:21:06},
  doi              = {10.4249/scholarpedia.4699},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:07},
}

@Misc{Wikipedia19CumulantWikipediaFree,
  author           = {{Wikipedia contributors}},
  note             = {[Online; accessed 24-July-2019]},
  title            = {Cumulant --- {Wikipedia}{,} The Free Encyclopedia},
  year             = {2019},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:07},
  url              = {https://en.wikipedia.org/w/index.php?title=Cumulant&oldid=891376504},
}

@Article{Bovier09shortcoursemean,
  author           = {Anton Bovier},
  title            = {A short course in mean field spin glasses},
  year             = {2009},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:07},
  url              = {https://wt.iam.uni-bonn.de/fileadmin/WT/Inhalt/people/Anton_Bovier/lecture-notes/erlangen.pdf},
}

@Book{Jolliffe11Principalcomponentanalysis,
  author           = {Jolliffe, Ian},
  publisher        = {Springer},
  title            = {Principal component analysis},
  year             = {2011},
  creationdate     = {2022-05-27T21:21:06},
  groups           = {rbm_diagrams},
  modificationdate = {2022-05-27T21:21:07},
}

@Article{Liao20randommatrixanalysis,
  author           = {Liao, Zhenyu and Couillet, Romain and Mahoney, Michael W},
  journal          = {Advances in Neural Information Processing Systems},
  title            = {A random matrix analysis of random fourier features: beyond the gaussian kernel, a precise phase transition, and the corresponding double descent},
  year             = {2020},
  pages            = {13939--13950},
  volume           = {33},
  creationdate     = {2022-06-01T23:08:38},
  file             = {:Liao20 - A Random Matrix Analysis of Random Fourier Features_ beyond the Gaussian Kernel, a Precise Phase Transition, and the Corresponding Double Descent.pdf:PDF},
  groups           = {bayesian chaos},
  modificationdate = {2022-06-01T23:08:49},
}

@Article{Liao19largedimensionalanalysis,
  author           = {Liao, Zhenyu and Couillet, Romain},
  journal          = {IEEE Transactions on Signal Processing},
  title            = {A large dimensional analysis of least squares support vector machines},
  year             = {2019},
  number           = {4},
  pages            = {1065--1074},
  volume           = {67},
  creationdate     = {2022-06-01T23:08:42},
  file             = {:Liao19 - A Large Dimensional Analysis of Least Squares Support Vector Machines.pdf:PDF},
  groups           = {bayesian chaos},
  modificationdate = {2022-06-01T23:08:58},
  publisher        = {IEEE},
}

@Article{VanGestel02Bayesianframeworkleast,
  author           = {Van Gestel, Tony and Suykens, Johan AK and Lanckriet, Gert and Lambrechts, Annemie and De Moor, Bart and Vandewalle, Joos},
  journal          = {Neural computation},
  title            = {Bayesian framework for least-squares support vector machine classifiers, Gaussian processes, and kernel Fisher discriminant analysis},
  year             = {2002},
  number           = {5},
  pages            = {1115--1147},
  volume           = {14},
  comment          = {They discuss the limits of classification for finite N in a Bayesian manner and get cited by RMT4ML},
  creationdate     = {2022-06-02T09:04:55},
  file             = {:Van Gestel02 - Bayesian Framework for Least Squares Support Vector Machine Classifiers, Gaussian Processes, and Kernel Fisher Discriminant Analysis.pdf:PDF},
  groups           = {bayesian chaos},
  modificationdate = {2022-06-02T09:05:32},
  publisher        = {MIT Press},
}

@Article{Jaeger01“echostate”approach,
  author           = {Jaeger, Herbert},
  journal          = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report},
  title            = {The “echo state” approach to analysing and training recurrent neural networks-with an erratum note},
  year             = {2001},
  number           = {34},
  pages            = {13},
  volume           = {148},
  creationdate     = {2022-06-08T23:22:44},
  file             = {:Jaeger01 - The “echo State” Approach to Analysing and Training Recurrent Neural Networks with an Erratum Note.pdf:PDF},
  groups           = {bayesian chaos},
  modificationdate = {2022-06-08T23:23:17},
  publisher        = {Bonn},
}

@Article{Hoerl70Ridgeregressionapplications,
  author           = {Hoerl, Arthur E and Kennard, Robert W},
  journal          = {Technometrics},
  title            = {Ridge regression: applications to nonorthogonal problems},
  year             = {1970},
  number           = {1},
  pages            = {69--82},
  volume           = {12},
  creationdate     = {2022-06-08T23:58:01},
  groups           = {bayesian chaos},
  modificationdate = {2022-06-08T23:58:01},
  publisher        = {Taylor \& Francis},
}

@Book{Schoelkopf02Learningkernelssupport,
  author           = {Sch{\"o}lkopf, Bernhard and Smola, Alexander J and Bach, Francis and others},
  publisher        = {MIT press},
  title            = {Learning with kernels: support vector machines, regularization, optimization, and beyond},
  year             = {2002},
  creationdate     = {2022-06-18T07:44:51},
  modificationdate = {2022-06-18T07:44:51},
}

@Article{Hofmann08Kernelmethodsmachine,
  author           = {Hofmann, Thomas and Sch{\"o}lkopf, Bernhard and Smola, Alexander J},
  journal          = {The annals of statistics},
  title            = {Kernel methods in machine learning},
  year             = {2008},
  number           = {3},
  pages            = {1171--1220},
  volume           = {36},
  creationdate     = {2022-06-18T07:52:31},
  file             = {:Hofmann08 - Kernel Methods in Machine Learning.pdf:PDF},
  modificationdate = {2022-06-18T07:52:37},
  publisher        = {Institute of Mathematical Statistics},
}

@Article{Bahri20Statisticalmechanicsdeep,
  author           = {Bahri, Yasaman and Kadmon, Jonathan and Pennington, Jeffrey and Schoenholz, Sam S and Sohl-Dickstein, Jascha and Ganguli, Surya},
  journal          = {Annual Review of Condensed Matter Physics},
  title            = {Statistical mechanics of deep learning},
  year             = {2020},
  number           = {1},
  volume           = {11},
  creationdate     = {2022-06-19T21:12:21},
  file             = {:Bahri20 - Statistical Mechanics of Deep Learning.pdf:PDF},
  modificationdate = {2022-06-19T21:12:26},
}

@Article{Thoppilan22LamdaLanguagemodels,
  author           = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal          = {arXiv preprint arXiv:2201.08239},
  title            = {Lamda: Language models for dialog applications},
  year             = {2022},
  creationdate     = {2022-06-19T21:20:57},
  file             = {:Thoppilan22 - Lamda_ Language Models for Dialog Applications.pdf:PDF},
  modificationdate = {2022-06-19T21:21:20},
}

@Book{Hardy34Inequalities,
  author           = {Hardy, GH and Littlewood, JE and Polya, G},
  publisher        = {Cambridge University Press},
  title            = {Inequalities},
  year             = {1934},
  creationdate     = {2022-06-24T09:40:26},
  groups           = {bayesian chaos},
  modificationdate = {2022-06-24T09:44:22},
}

@Book{Sutton18Reinforcementlearningintroduction,
  author           = {Sutton, Richard S. and Barto, Andrew G.},
  publisher        = {MIT press},
  title            = {Reinforcement learning: {An} introduction},
  year             = {2018},
  creationdate     = {2022-07-04T09:26:09},
  file             = {:Sutton18 - Reinforcement Learning_ an Introduction.pdf:PDF},
  modificationdate = {2022-07-04T09:32:24},
  shorttitle       = {Reinforcement learning},
}

@Article{Schultz98Predictiverewardsignal,
  author           = {Schultz, Wolfram and Neurophysiol, J. and Schultz, Wolfram},
  journal          = {J Neurophysiol},
  title            = {Predictive reward signal of dopamine neurons},
  year             = {1998},
  abstract         = {You might find this additional info useful... This article cites 264 articles, 77 of which you can access for free at:},
  comment          = {TD learning in monkeys},
  creationdate     = {2022-07-04T09:40:20},
  file             = {:Schultz98 - Predictive Reward Signal of Dopamine Neurons.pdf:PDF},
  modificationdate = {2022-07-04T09:40:55},
}

@Article{VanVreeswijk96Chaosneuronalnetworks,
  author           = {Van Vreeswijk, Carl and Sompolinsky, Haim},
  journal          = {Science},
  title            = {Chaos in neuronal networks with balanced excitatory and inhibitory activity},
  year             = {1996},
  number           = {5293},
  pages            = {1724--1726},
  volume           = {274},
  creationdate     = {2022-07-04T11:56:10},
  file             = {:Van Vreeswijk96 - Chaos in Neuronal Networks with Balanced Excitatory and Inhibitory Activity.pdf:PDF},
  modificationdate = {2022-07-04T11:56:18},
  publisher        = {American Association for the Advancement of Science},
}

@Article{Dubreuil22rolepopulationstructure,
  author           = {Dubreuil, Alexis and Valente, Adrian and Beiran, Manuel and Mastrogiuseppe, Francesca and Ostojic, Srdjan},
  journal          = {Nature Neuroscience},
  title            = {The role of population structure in computations through neural dynamics},
  year             = {2022},
  pages            = {1--12},
  creationdate     = {2022-07-04T17:23:37},
  file             = {:Dubreuil22 - The Role of Population Structure in Computations through Neural Dynamics.pdf:PDF},
  modificationdate = {2022-07-04T17:24:03},
  priority         = {prio2},
  publisher        = {Nature Publishing Group},
}

@Article{Neyman33problemmostefficient,
  author           = {Neyman, Jerzy and Pearson, Egon Sharpe},
  journal          = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  title            = {On the problem of the most efficient tests of statistical hypotheses},
  year             = {1933},
  number           = {694-706},
  pages            = {289--337},
  volume           = {231},
  creationdate     = {2022-07-06T08:42:47},
  file             = {:Neyman33 - IX. on the Problem of the Most Efficient Tests of Statistical Hypotheses.pdf:PDF},
  modificationdate = {2022-07-06T08:46:48},
  publisher        = {The Royal Society London},
}

@Article{Saxe13Exactsolutionsnonlinear,
  author           = {Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal          = {arXiv preprint arXiv:1312.6120},
  title            = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  year             = {2013},
  creationdate     = {2022-07-07T18:40:31},
  file             = {:Saxe13 - Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks.pdf:PDF},
  modificationdate = {2022-07-07T18:40:39},
}

@InProceedings{MacQueen67Classificationanalysismultivariate,
  author           = {MacQueen, J},
  booktitle        = {5th Berkeley Symp. Math. Statist. Probability},
  title            = {Classification and analysis of multivariate observations},
  year             = {1967},
  pages            = {281--297},
  creationdate     = {2022-07-08T11:15:30},
  modificationdate = {2022-07-08T11:15:30},
}

@Article{Schrittwieser20Masteringatarigo,
  author           = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal          = {Nature},
  title            = {Mastering atari, go, chess and shogi by planning with a learned model},
  year             = {2020},
  number           = {7839},
  pages            = {604--609},
  volume           = {588},
  creationdate     = {2022-07-12T12:15:44},
  file             = {:Schrittwieser20 - Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.pdf:PDF},
  modificationdate = {2022-07-12T12:15:49},
  publisher        = {Nature Publishing Group},
}

@Article{Mnih15Humanlevelcontrol,
  author           = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin Riedmiller and Andreas K. Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
  journal          = {Nature},
  title            = {Human-level control through deep reinforcement learning},
  year             = {2015},
  month            = {feb},
  number           = {7540},
  pages            = {529--533},
  volume           = {518},
  creationdate     = {2022-07-13T14:33:45},
  doi              = {10.1038/nature14236},
  file             = {:Mnih15 - Human Level Control through Deep Reinforcement Learning.pdf:PDF},
  modificationdate = {2022-07-13T14:35:05},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{LeCun22PathTowardsAutonomous,
  author           = {LeCun, Yann},
  title            = {A Path Towards Autonomous Machine Intelligence Version 0.9. 2, 2022-06-27},
  year             = {2022},
  creationdate     = {2022-07-13T14:36:07},
  file             = {:LeCun22 - A Path Towards Autonomous Machine Intelligence Version 0.9. 2, 2022 06 27.pdf:PDF},
  modificationdate = {2022-07-13T14:36:11},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectory:/Users/jan/OneDrive - rwth-aachen.de/literature;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:RNFlows JC\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:PDLT\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:R32 Bosch\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:bayesian chaos\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:misc\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:RL\;0\;0\;0x8a8a8aff\;\;\;;
1 StaticGroup:JAJ\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:rbm_diagrams\;0\;1\;0x8a8a8aff\;\;\;;
}
